[{"title":"How to read","url":"/2022/07/01/Read/","content":"\n> An introduction to research paper reading\n\n# 批判阅读\n\n- 如果作者是尝试解决某个问题\n  - 那他们要解决的这个问题是正确的么？\n  - 有没有更简单的解法？\n  - 答案有什么局限性？\n- 如果作者呈现数据\n  - 他们是否有正确的数据来支撑论点？\n  - 他们是否以正确的方式来组织这些数据？\n  - 他们是否以合理的方式来解释了这些数据？\n  - 是否还有其他更具说服力的数据集？\n- 作者的假设合理吗？\n  - 逻辑清晰且公正吗？\n  - 理由有没有漏洞？\n\n\n# 创造性阅读\n\n- 文章还有哪些点是作者没想到的\n- 我是否可以对那些地方进行改进，从而提升效果\n- 能不能把这篇论文和其他的论文关联起来，从而产生新的想法，可以支撑下一阶段的研究\n\n如果真正想理解论文，可以写一个摘要，最好做一个报告。当把东西写下来或者说出来，才能说明我们真正理解了。\n\n# 问题清单\n\n- 论文的核心观点是什么？\n- 主要的局限性是什么？\n- 代码和数据是不是可用？\n- 这是一个好的想法吗？是否违反直觉？\n- 论文的贡献是否有意义？\n- 论文中的实验是否足够好？还可以调优吗？\n- 有没有错过什么相关论文么？\n- 对我的工作和产出有何帮助么？\n- 这是一篇值得关注的论文么？\n- 其他的人对这篇论文有何看法呢？\n- 这个研究领域的领头人是谁呢？\n- 如果有机会见到作者，我应该问作者什么问题？\n\n> 这里我引用两个人说的话，一个是杨振宁先生，他说他曾经看到过几千个研究者，有的10年后非常成功，有的却失败了，这并不是因为成功者更聪明，而是因为成功者找到了正确的方向，知道该做什么；还有就是上周看到了祥雨朋友圈分享了一个观点，我觉得还蛮有意思的：真正做一些伟大的东西，往往都需要很好的直觉——不是数学，不是理论推导，而是你根据历史脉络和广度得到的一些思考。带着这样的思考和信念去做科研其实是更简单的：（做科研）其实是一个Easy模式，而不是Hard模式。 --huhan\n\n> 或许你永远不知道你以前读过的书能在什么时候能够派上用场，但请保持阅读，因为阅读的过程也是在你大脑中建立认知的过程。\n>\n> 深度阅读论文，要敢于对论文质疑，质疑论文作者的研究方法、思路、技巧。还要设身处地去想：如果我来写这篇论文，我能用什么方法。 --Harry\n\n[沈向洋博士在全球创新学院（GIX）进行的直播“You are how you read”视频回放](https://www.bilibili.com/video/BV1df4y1m74k?spm_id_from=333.880.my_history.page.click&vd_source=1d323c0fb399b7e3d65130870e2e897b)\n","tags":["paper"]},{"title":"EdgeNext","url":"/2022/06/23/EdgeNext/","content":"\n>  目前最新的轻量级网络，整体结构清晰，代码可复现性强，其中的设计思路值得学习。在通道层面做自注意力将复杂度降低到线性的，同时利用深度可分离卷积构建空间上的上下文关系，分开建模两部分的信息。实验效果提升显著，保持高精度的同时大大降低了延迟和运算。\n\n#Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications\n\n## Overall Architecture\n\n![image-20220623154946031](EdgeNext/image-20220623154946031.png)\n\n## Conv Encoder\n\n```python\nclass ConvEncoder(nn.Module):\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=4, kernel_size=7):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(expan_ratio * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n        return x\n```\n\n## SDTA Encoder\n\n```python\nclass SDTAEncoder(nn.Module):\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=4,\n                 use_pos_emb=True, num_heads=8, qkv_bias=True, attn_drop=0., drop=0., scales=1):\n        super().__init__()\n        # width：每个分支的对应通道数\tscales：几个分支\n        width = max(int(math.ceil(dim / scales)), int(math.floor(dim // scales)))\n        self.width = width\n        if scales == 1:\n            self.nums = 1\n        else:\n            self.nums = scales - 1\t#需要的卷积数为分支数-1\n        convs = []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, kernel_size=3, padding=1, groups=width))\n        self.convs = nn.ModuleList(convs)\n\n        self.pos_embd = None\n        if use_pos_emb:\n            self.pos_embd = PositionalEncodingFourier(dim=dim)\n        self.norm_xca = LayerNorm(dim, eps=1e-6)\n        self.gamma_xca = nn.Parameter(layer_scale_init_value * torch.ones(dim),\n                                      requires_grad=True) if layer_scale_init_value > 0 else None\n        self.xca = XCA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)  # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()  # TODO: MobileViT is using 'swish'\n        self.pwconv2 = nn.Linear(expan_ratio * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n\n        spx = torch.split(x, self.width, 1)\n        # 以Stange2为例,self.nums=1, i只取0\n        for i in range(self.nums):\t\n            if i == 0:\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            if i == 0:\n                out = sp\n            else:\n                out = torch.cat((out, sp), 1)\n        x = torch.cat((out, spx[self.nums]), 1)\n        # XCA\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H * W).permute(0, 2, 1)\n        if self.pos_embd:\n            pos_encoding = self.pos_embd(B, H, W).reshape(B, -1, x.shape[1]).permute(0, 2, 1)\n            x = x + pos_encoding\n        x = x + self.drop_path(self.gamma_xca * self.xca(self.norm_xca(x)))\n        x = x.reshape(B, H, W, C)\n\n        # Inverted Bottleneck\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n\n        return x\n\n#通道方向的自注意力\nclass XCA(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\t\t\n        # (H*W, C) -> (C, H*W)\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        # -------------------\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n        # ------------------\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'temperature'}\n```\n\n## EdgeNeXt\n\n```python\nclass EdgeNeXt(nn.Module):\n    def __init__(self, in_chans=3, num_classes=1000,\n                 depths=[3, 3, 9, 3], dims=[24, 48, 88, 168],\n                 global_block=[0, 0, 0, 3], global_block_type=['None', 'None', 'None', 'SDTA'],\n                 drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1., expan_ratio=4,\n                 kernel_sizes=[7, 7, 7, 7], heads=[8, 8, 8, 8], use_pos_embd_xca=[False, False, False, False],\n                 use_pos_embd_global=False, d2_scales=[2, 3, 4, 5], **kwargs):\n        super().__init__()\n        for g in global_block_type:\n            assert g in ['None', 'SDTA']\n        if use_pos_embd_global:\n            self.pos_embd = PositionalEncodingFourier(dim=dims[0])\n        else:\n            self.pos_embd = None\n        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n        for i in range(4):\n            stage_blocks = []\n            for j in range(depths[i]):\n                if j > depths[i] - global_block[i] - 1:\n                    if global_block_type[i] == 'SDTA':\n                        stage_blocks.append(SDTAEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                        expan_ratio=expan_ratio, scales=d2_scales[i],\n                                                        use_pos_emb=use_pos_embd_xca[i], num_heads=heads[i]))\n                    else:\n                        raise NotImplementedError\n                else:\n                    stage_blocks.append(ConvEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                    layer_scale_init_value=layer_scale_init_value,\n                                                    expan_ratio=expan_ratio, kernel_size=kernel_sizes[i]))\n\n            self.stages.append(nn.Sequential(*stage_blocks))\n            cur += depths[i]\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # Final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head_dropout = nn.Dropout(kwargs[\"classifier_dropout\"])\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):  # TODO: MobileViT is using 'kaiming_normal' for initializing conv layers\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (LayerNorm, nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_features(self, x):\n        x = self.downsample_layers[0](x)\n        x = self.stages[0](x)\n        if self.pos_embd:\n            B, C, H, W = x.shape\n            x = x + self.pos_embd(B, H, W)\n        for i in range(1, 4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n\n        return self.norm(x.mean([-2, -1]))  # Global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(self.head_dropout(x))\n        return x\n```\n\n## Experiments\n\n![image-20220623164213601](EdgeNext/image-20220623164213601.png)\n\n![image-20220623164312032](EdgeNext/image-20220623164312032.png)\n\n> [论文地址](https://readpaper.com/pdf-annotate/note?pdfId=699460569366302720&noteId=699501058328915968)\n>\n> [source code](https://github.com/mmaaz60/EdgeNeXt)\n\n","tags":["paper"],"categories":["paper"]},{"title":"mmcv","url":"/2022/06/19/mmcv/","content":"\n# mmcv功能概览\n\n![image-20220619194629220](mmcv/image-20220619194629220.png)\n\n- ​\t配置文件的可读性比原始的好，直接反映了配置和实现的关系。上图左中`type`表示此模型为`RetinaNet`，后续字典都是其属性。\n\n- ​    配置文件通过继承更清晰其中变动之处，不用每次去找不同之处了。\n\n- ​\t不仅支持`python`格式的配置文件，也支持`yaml`等格式，但灵活性没那么好。\n\n  ![image-20220619195356822](mmcv/image-20220619195356822.png)\n\n配置文件中的各种参数代表了`RetinaHead`在构造时对应的值。其中绿色虚线框对应`**kwargs`，这些属性并不是给`RetinaHead`使用的，而是给其父类的。我们在`RetinaHead`中会调用父类的方法，利用`**kwargs`将父类对应的属性初始化\n\n![image-20220619200037241](mmcv/image-20220619200037241.png)\n\n`RetinaHead`仅仅只是`RetinaNet`中的一个字段，即传入的最外层传入一个`type`内层都是带`type`结构的子结构字典。我们只要保证在具体类的实现中，内部有去解析这些字典。对应图中`RetinaNet` 父类`SingleStangeDetector`中通过`build_backbone`在内部完成了对应属性的实例化\n\n![image-20220619201554085](mmcv/image-20220619201554085.png)\n\n`_base_`申明一个配置文件列表，会将所有配置文件中定义的变量加载进来。其中`model`是对父类中内部字段**进行修改而不是覆盖**。\n\n只有在其中加入`_delete_`属性才会覆盖父类中对应字段\n\n配置文件通过一个列表来表示，但不能显示的知道配置文件中到底由哪些组件构成。vscode插件`Config View`可以解决这个问题\n\n## Config与Registry\n\n![image-20220619203517389](mmcv/image-20220619203517389.png)\n\n从`Config`中取对应字段的配置信息**还没有将其变成具体的实例只有解析功能**，要通过`Registry`将对应实例构建出来。\n\n![image-20220619204730643](mmcv/image-20220619204730643.png)\n\n根据`type`字段将`cfg`粗略分为两个类型。`type`类型：**训练时该变量的实例化后类型，其余字段为该变量实例化所需的参数。**`Registry `会根据`type` 和其他参数将变量实例化。\n\n`mmcv`训练最外层的抽象是`runner`，其有非常多的属性，如果在配置文件中写入runner，会导致嵌入很多层级的字典，阅读不便。所以在可读性和一一映射关系上做了取舍。对于`dataloader`和`dataset`为了防止文件嵌套层级过深，只将一些显而易见的属性写在最外层，不将这些暴露给用户 。\n\n![image-20220619210213182](mmcv/image-20220619210213182.png)\n\n![image-20220619210605888](mmcv/image-20220619210605888.png)\n\n- 左侧的方式每次构建完模型后都要手动向`model_factory`中加入一个字段。\n\n- 右侧是个`Registry`的最简形式。每个`Registry`实例都会有一个`module_dict`字典，利用一个装饰器实现将字符串与模型类的映射会自动添加到`module_dict`中，多一个模型只需多一行注册，便于维护。\n\n- 注册功能的具体用法：一种是装饰器的写法，只要对应脚本被执行（本身在运行该脚本或者在运行其他脚本时`import`该脚本，触发任意一种就完成注册行为）\n\n  一种是注册外部模块的写法，将其作为函数使用。\n\n![image-20220619211754014](mmcv/image-20220619211754014.png)\n\n`register_module`方法中可以传参，意味着不仅可以注册自己写的模块还可以注册外部的模块。\n\n![image-20220619212028484](mmcv/image-20220619212028484.png)\n\n正因为`registry`可以注册外部模块我们才能在配置文件中调用到这些模块。\n\n以上注册代码都写在某个模块中 ，要注册这些模块必须要运行对应脚本。\n\n![image-20220620201643776](mmcv/image-20220620201643776.png)\n\n![image-20220620201933637](mmcv/image-20220620201933637.png)\n\n测试代码的注册过程在脚本执行过程就完成了注册，训练中注册过程可通过`import`。\n\n侵入式注册：这种链式的行为导致`mmdet`更新后可能会与本地分支产生冲突。非侵入式注册则的方式则不需要修改对应的算法库。\n\n![image-20220620203346466](mmcv/image-20220620203346466.png)\n\n![image-20220620203652989](mmcv/image-20220620203652989.png)\n\n![image-20220620204435092](mmcv/image-20220620204435092.png)\n\n> [Config&Registry背后的故事ppt](https://openmmlab.feishu.cn/file/boxcnxBOiTaNOjPojDihE9vBDRd)\n>\n> [视频讲解](https://www.bilibili.com/video/BV19Z4y1q7Q5?spm_id_from=444.41.list.card_archive.click&vd_source=1d323c0fb399b7e3d65130870e2e897b)\n","tags":["code"],"categories":["source code"]},{"title":"DETR","url":"/2022/06/10/DETR/","content":"\n# End-to-End Object Detection with Transformers\n\n> 第一个端到端的检测器，没有了anchor和nms后处理，简化了流程。不过训练时间长，小物体检测效果差。DETR的成功主要还是Transformer的成功，之前也试过基于集合的目标函数、Encoder-Decoder的架构效果都不好，主要原因是特征不够好。\n\n![image-20220610100946952](DETR/image-20220610100946952.png)\n\n**主要贡献：** \t\n\n- **新的目标函数**（通过二分图匹配的方式输出一组预测，替代了原来的`nms`）\n\n- ` Encoder-Decoder Architecture`的架构 (预测是并行出框，不同于以往的自回归预测)\n\n  ## Introduce\n\n![image-20220610102211789](DETR/image-20220610102211789.png)\n\n**训练过程如下：**\n\n- `CNN`：抽取特征\n- `Transformer encoder`：每个特征会和图中其他所有特征交互，这样网络大概知道哪块是哪个物体，对同一个物体只出一个框，所以这种全局建模的方式有利于移除冗余的框。\n- `Decoder` ：生成预测框\n\n- 预测框和GT框做匹配：将这个过程看成集合预测的问题，在匹配上的框上作loss。\n\n推理过程中置信度大于0.7的物体才会被保留\n\n## Model\n\n###  Set prediction loss\n\n![image-20220610110729432](DETR/image-20220610110729432.png)\n\n![image-20220610111802283](DETR/image-20220610111802283.png)\n\n二分图匹配的例子：如何分配一些工人做一些工作使最后支出最小? 最优二分图匹配即最后有唯一解达成目标且成本最低。遍历算法来解决复杂度太高，常用匈牙利算法即`Scipy`包中的`linear-sum-assignment`函数来完成，该函数的输入就是`Cost matrix`, 输出即最优排列。可以将100个预测框视为`a、b、c`，GT框视为`x、y、z` ，检测中`Cost matrix`是损失值。最终输出的是与`GT`唯一匹配的预测框。\n\n![image-20220610112114389](DETR/image-20220610112114389.png)\n\n![image-20220610112446959](DETR/image-20220610112446959.png)\n\n![image-20220610112533981](DETR/image-20220610112533981.png)\n\n完成了这个最优匹配操作，就可以计算一个真正的目标函数loss，从而更新模型参数。\n\n### DETR architecture\n\n![image-20220610112641919](DETR/image-20220610112641919.png)\n\n### Visualizing\n\n![image-20220610122822730](DETR/image-20220610122822730.png)\n\n![image-20220610124936036](DETR/image-20220610124936036.png)\n\n`Encoder`越深不同物体的区分性越好\n\n![image-20220610122850085](DETR/image-20220610122850085.png)\n\n**可视化结果：** 仅使用`Transformer Encoder`图像中的物体已经有很好的区分了，再此基础上做`Decoder`后效果更佳。编解码一个都不能少，`Encoder`在学习一个全局的特征，将物体与物体区分开；`Decoder`在前面的基础上只需对头、尾巴等对象边界特征进行学习以解决遮挡问题来更好的区分物体\n\n![image-20220610131413686](DETR/image-20220610131413686.png![image-20220610132117871](DETR/image-20220610132117871.png)\n\n上图为`object query`的可视化，每个正方形代表一个`object query`，替代了anchor的生成机制，不同在于它是自己学的。以第一个为例，该`object query`学到最后，会问每个输入图片在左下角有没有看到小物体，在中间有没有看到横向的大物体，有的话告诉我。这100个`object query`相当于100个不停问问题的人，得到的答案就是目标框。\n\n```python\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet50\n\nclass DETR(nn.Module):\n    def __init__(self, num_classes, hidden_dim, nheads,8 num_encoder_layers, num_decoder_layers):\n        super().__init__()\n        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n        self.transformer = nn.Transformer(hidden_dim, nheads,14 num_encoder_layers, num_decoder_layers)\n        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n        self.linear_bbox = nn.Linear(hidden_dim, 4)\n        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        \n    def forward(self, inputs):\n        x = self.backbone(inputs)\n        h = self.conv(x)\n        H, W = h.shape[-2:]\n        pos = torch.cat([26 self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1)\n        h = self.transformer(pos + h.flatten(2).permute(2, 0, 1), self.query_pos.unsqueeze(1))\n        return self.linear_class(h), self.linear_bbox(h).sigmoid()\n    \ndetr = DETR(num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)\ndetr.eval()\ninputs = torch.randn(1, 3, 800, 1200)\nlogits, bboxes = detr(inputs)\n        \n\n```\n\n","tags":["paper"],"categories":["paper"]},{"title":"Vision GNN","url":"/2022/06/06/VisionGNN/","content":"\n# Vision GNN: An Image is Worth Graph of Nodes\n\n> 将图结构应用于图像领域，将VIT中的patches视作nodes。该网络主要由`GCN`和`FFN`组成。其中`GCN`模块来聚合、处理图结构信息；`FFN`模块对节点特征作变化，获取节点信息的多样性。[论文链接](https://readpaper.com/pdf-annotate/note?noteId=693400681528512512&pdfId=4630291798619602945)\n\n![image-20220606230827512](VisionGNN/image-20220606230827512.png)\n\n   如上图所示，节点之间的连接由其内容确定，不受位置关系限制。网格、序列都可以看成是图的特例。\n\n![image-20220607131156703](VisionGNN/image-20220607131156703.png)\n\n上图是图片如何表示成图的一个例子。蓝色点 表示`adjacent matrix`，白色点表示无连接；通常是很大的`sparse matrix`。\n\n> 图片出处：[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n\n#  How to transform an image to a graph\n\n##   VIG\n\n![image-20220606231533521](VisionGNN/image-20220606231533521.png)\n\n   网络的关键在于：如何将图像表示为图结构、如何学习图结构中的视觉表征\n\n### Graph Representation of Image\n\n![image-20220606232134301](VisionGNN/image-20220606232134301.png)\n\n此处`K取9~15`效果最佳\n\n### GCN\n\n  图卷积通过聚合相邻节点的特征来实现节点之间的信息交换。\n\n![image-20220606233416153](VisionGNN/image-20220606233416153.png)\n\n   具体的说图卷积分为聚合和更新两个操作。\n\n- ` aggregation operation`: 操作通过聚合邻接节点的特征来计算当前节点的表示\n- ` multi-head update operation`: 将聚合后的特征分为`h`个头，每个头对应不同的更新权重，有利于特征的多样性表达。\n\n![image-20220606234233716](VisionGNN/image-20220606234233716.png)\n\n​    `GCN`在网络变深后会有**过平滑现象**，这会降低节点特征的区分度，导致视觉识别性能下降。本文在**图卷积后**的每个节点内引入了更多的特征变化，促增加节点特征多样性。 \n\n![image-20220606235817431](VisionGNN/image-20220606235817431.png)\n\n   本文对应的解决策略如公式6所示。**为了进一步提高特征转换能力和缓解过平滑现象，本文在每个节点上使用`FFN`**，过程如下所示。\n$$\nZ = σ (Y W1 ) W2 + Y\n$$\n   基于本节的`graph representation of images` 和`ViG block`，VIG网络可以随着网络的加深，保持特征的多样性，从而学习判别性的表示。\n\n##  Experiments\n\n![image-20220607124600794](VisionGNN/image-20220607124600794.png)\n\n![image-20220607124657067](VisionGNN/image-20220607124657067.png)\n\n![image-20220607124921992](VisionGNN/image-20220607124921992.png)\n\n### Visualization\n\n![image-20220607125000107](VisionGNN/image-20220607125000107.png)\n\n   **可视化结果：浅层邻接节点的选取基于低级、局部特征如：颜色和纹理，深层中邻接节点的选取更依赖于语义等高级特征。**\n\n### Pseudocode\n\n![image-20220607130309524](VisionGNN/image-20220607130309524.png)\n\n<img src=\"VisionGNN/image-20220607130529777.png\" alt=\"image-20220607130529777\" style=\"zoom:150%;\" />\n\n<img src=\"VisionGNN/image-20220607130639174.png\" alt=\"image-20220607130639174\" style=\"zoom:150%;\" />\n","tags":["paper"],"categories":["paper"]},{"title":"data processing","url":"/2022/06/05/detection/","content":"\n# 数据格式的转换\n\n## 1. VOC\n\n- `VOC/`\n  - `Annotations/`\n    - `.xml`\n  - `ImageSets/`\n    - `Main/`\n      - `train.txt`\n      - `test.txt`\n      - `val.txt`\n      - `trainval.txt`\n  - `JPEGImages/`\n    - `.jpg`\n\n## 2. COCO\n\n- `coco/`\n  - `annotations/`\t\t#该文件下共六种json文件，此处只取两种\n    \n    - `instances_train2017.json`    # instances：为目标检测与实例分割的标注文件\n    - `instances_val2017.json`\n  \n  > json文件主要包含以下几个字段\n  >\n  > {\n  >  \"info\": info, # dict 数据集描述\n  >\n  >   \"licenses\": [license], # list ，内部是dict\n  >\n  >   \"images\": [image], # 主要包括file_name, height, width，id是图片的唯一标识\n  >\n  >   \"annotations\": [annotation], # list ，主要包括检测与分割的标注信息\n  >\n  >   \"categories\": # list ，内部是dict id,  主要字段包括image_id,  category_id,  segmentation,  num_keypoints,  area,  iscrowd,  keypoints，bbox等\n  > }\n  \n  - `images/`\n    - `train2017/`\n      - `.jpg`\n    - `val2017`\n      - `.jpg`\n\n## 3. Labelme\n\n- `labelme/`\n  - `.json`\n\n  - `.jpg`\n\n\n`labelme2coco`:https://github.com/spytensor/prepare_detection_dataset/blob/master/labelme2coco.py\n\n## 4. YOLO\n\n- `yolo/`\n  - `images`\n    - `.txt` \t\t# 类id、x_center、y_center、w、h（真实像素值除以图片的高和宽之后的值） \n  - `labels`\n    - `.jpg`\n\n`coco2txt`\n\n```python\n# COCO 格式的数据集转化为 YOLO 格式的数据集\n# --json_path 输入的json文件路径\n# --save_path 保存的文件夹名字，默认为当前目录下的labels\n\nimport os\nimport json\nfrom tqdm import tqdm\nimport argparse\n\nparser = argparse.ArgumentParser()\n# 这里根据自己的json文件位置，换成自己的就行\nparser.add_argument('--json_path',\n                    default=r'your path', type=str,\n                    help=\"input: coco format(json)\")\n# 这里设置.txt文件保存位置\nparser.add_argument('--save_path', default=r'your path', type=str,\n                    help=\"specify where to save the output dir of labels\")\narg = parser.parse_args()\n\ndef convert(size, box):\n    dw = 1. / (size[0])\n    dh = 1. / (size[1])\n    x = box[0] + box[2] / 2.0\n    y = box[1] + box[3] / 2.0\n    w = box[2]\n    h = box[3]\n    # round函数确定(xmin, ymin, xmax, ymax)的小数位数\n    x = round(x * dw, 6)\n    w = round(w * dw, 6)\n    y = round(y * dh, 6)\n    h = round(h * dh, 6)\n    return (x, y, w, h)\n\n\nif __name__ == '__main__':\n    json_file = arg.json_path  # COCO Object Instance 类型的标注\n    ana_txt_save_path = arg.save_path  # 保存的路径\n\n    data = json.load(open(json_file, 'r'))\n    if not os.path.exists(ana_txt_save_path):\n        os.makedirs(ana_txt_save_path)\n\n    id_map = {}  # coco数据集的id不连续！重新映射一下再输出！\n    with open(os.path.join(ana_txt_save_path, 'classes.txt'), 'w') as f:\n        # 写入classes.txt\n        for i, category in enumerate(data['categories']):\n            f.write(f\"{category['name']}\\n\")\n            id_map[category['id']] = i\n    # print(id_map)\n    # 这里需要根据自己的需要，更改写入图像相对路径的文件位置。\n    list_file = open(os.path.join(ana_txt_save_path, 'train2017.txt'), 'w')\n    for img in tqdm(data['images']):\n        filename = img[\"file_name\"]\n        img_width = img[\"width\"]\n        img_height = img[\"height\"]\n        img_id = img[\"id\"]\n        head, tail = os.path.splitext(filename)\n        ana_txt_name = head + \".txt\"  # 对应的txt名字，与jpg一致\n        f_txt = open(os.path.join(ana_txt_save_path, ana_txt_name), 'w')\n        for ann in data['annotations']:\n            if ann['image_id'] == img_id:\n                box = convert((img_width, img_height), ann[\"bbox\"])\n                f_txt.write(\"%s %s %s %s %s\\n\" % (id_map[ann[\"category_id\"]], box[0], box[1], box[2], box[3]))\n        f_txt.close()\n```\n","tags":["code"]},{"title":"MUxin","url":"/2022/05/22/Mu/","content":"\n# 引\n\n> 鲜有良朋，贶也永叹——《诗经》\n>\n> （“贶”，音同“况”，赐的意思），取《诗经》，意思是少有朋友和我长叹长谈了。\n\n> 微神之躬，胡为乎泥中——《诗经》\n>\n> 若不是为了你的缘故，我不会在泥中打滚\n\n> 凡是令我倾心的书，都分辨不清是我在理解它呢还是它在理解我。\n\n>我爱的物、事、人，是不太提的。我爱音乐，不太听的。我爱某人，不太去看他的。现实生活中遇到他，我一定远远避开他。这是我的乖僻，是为了更近人情。\n\n>哲学、科学，拆了宗教的台，哲学成了控告宗教的原告，科学在旁边做证人。艺术，做了无神论的最高榜样，不仅否认神，还取代了神，不仅取消了神的诺言，还自己创造诺言，立即在现世兑现。\n\n>要相信相对真实。夫妻的意思，就是凭道义、义务，共同生活，是守约，不能去要求爱情。爱情，是青春、美貌、神秘。夫妻呢，是有福同享、有难同当。\n>\n>真够悲凉的，鉴于先生终生一人，此句我只赞同一半。\n\n>要从远处回，从高处下，从深处出。\n\n> 生离等于死别\t庸凡不知\t忽生离而恸死别\n\n> 尚无知识、经验、能力时，要竭力自尊，抗御邪恶，真是艰辛。\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t**——待续**\n","tags":["literature"]},{"title":"Image_compress","url":"/2022/05/19/imagecompress/","content":"\n# JPEG 不可思议的压缩率\n\n> 以JPEG图像压缩算法为例，讲述信号处理的核心思想和主题（还讲了颜色空间、`YCbCr`、色度子采样、离散余弦变换、量化和无损编码等细节。)\n\n## 引子\n\n![image-20220519152830270](imagecompress/image-20220519152830270.png)\n\n**核心问题：**图片压缩过程中哪类信息可以去除，怎么去除\n\n![image-20220519153141089](imagecompress/image-20220519153141089.png)\n\n科学家发现人眼对亮度更敏感对颜色没那么敏感——JPEG压缩方案考虑了这个特点——想要知道怎么做必须深入探索色彩空间领域。\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/colorspace.png\"></div>\n        <div style=\"display:table-cell; width:27%;\"><img src=\"imagecompress/color.png\"></div>\n    </div>\n</div>\n\nRGB色彩空间有个特点：如果沿着对角线走从原点走到（255，255，255），就能得到逐渐变亮的颜色，这些点构成的线定义了所有可能的灰阶颜色，这是衡量亮度最直接的方式。将亮度独立出来的想法是另一个色彩空间的基石`YCbCr` ,`Y`分量衡量的是图片的“流明”或亮度，`Cb`和`Cr`存储的是颜色信息。`Y`可以视为一个单独的纵轴，值越大表明亮度越大。JPEG使用的这种色彩空间的原因是让我们能直接调用最适合人眼识别的颜色。\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/1.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:26%;\"><img src=\"imagecompress/2.png\"></div>\n    </div>\n</div>\n\n\n由于人对亮度敏感度高，其中一个压缩原图的思路就是缩减`Cb`和`Cr`分量的采样数，但把亮度分量全保留。这个技术被称为“色度下采样/色度抽样”。\n\n\n\n------\n\n\n\n## 色度下采样/色度抽样\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/3.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:55%;\"><img src=\"imagecompress/4.png\"></div>\n    </div>\n</div>\n\n`色度下采样`的具体操作是对原图种四个像素点求平均得到一个值；`色度抽样`通常选左上角像素值代表整个2×2区域的颜色，待色彩分量的采样数缩减后就可以和亮度分量合并如上右图所示。在这里色彩分量保持16个像素，这就得到了抽样后的图像。\n\n<img src=\"imagecompress/image-20220519161938690.png\" alt=\"image-20220519161938690\"  />\n\n至此距离JPEG 95%的压缩率仍有距离，需要考虑其他方面\n\n![image-20220519162214777](imagecompress/image-20220519162214777.png)\n\n## Y\n\n下面的压缩环节我们关注`Y`通道,本节的核心原理同样可以用于色彩分量\n\n### DCT概述\t\n\n我们需要以完全不同的视角来看待图片，有一个看待图片的角度是将图片视作“信号”\n\n![image-20220519162908961](imagecompress/image-20220519162908961.png)\n\n取出图片中若干像素构成的一行，使我们得以讨论图片里的“频率分量”\n\n![image-20220519163136283](imagecompress/image-20220519163136283.png)\n\n![image-20220519163255418](imagecompress/image-20220519163255418.png)\n\n人类视觉系统通常对图片的高频细节不敏感，因此JPEG会有策略地去除图片中不太重要、较少见的高频信号，达到更大的压缩率。**那么如何提取频率分量呢？**\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:33%\"><img src=\"imagecompress/5.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:30%;\"><img src=\"imagecompress/6.png\"></div>\n    </div>\n</div>\n\n\n先将这8个像素当成是某种信号，`DCT`将原始信号的**样本点**作为输入。\n\n![image-20220519165127117](imagecompress/image-20220519165127117.png)\n\n![image-20220519165454818](imagecompress/image-20220519165454818.png)\n\n用“系数”来描述`DCT`的输出,==**这些系数描述了组成原始信号的不同频率余弦波的权重**== 。本质是求出某个特定的余弦波应在信号里包含多少。\n\n可以把这个过程类比为将复杂的信号拆分成简单余弦波的加权求和，**那么到底使用了哪些余弦波，波如何和图中的像素建立关联？**以下取cos(x)为例\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:40%\"><img src=\"imagecompress/7.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:26%;\"><img src=\"imagecompress/8.png\"></div>\n    </div>\n</div>\n\n\n\n为了与之前的例子保持一致，我们需要在余弦波中采8个点做样本\n\n![image-20220519165454818](imagecompress/9.png)\n\n输出表明对输入有贡献的余弦波只有一个，看似合理因为此时的输入本身就取自一个余弦波。**==改变余弦波的振幅==**发现`DCT`系数`X1`同比变化，说明其实际担任了余弦曲线中权重的角色。**那么如何将其与图像联系起来？**\n\n![image-20220519172027023](imagecompress/image-20220519172027023.png)\n\n**构建余弦波到图像的映射：**要让余弦波到图像的映射更合理可以把像素值平移128个单位`(0,255)->(-128,127)`此时，像素变化的大小和方向便反映到原始余弦波的振幅上即`DCT`的系数。\n\n![image-20220519172509224](imagecompress/image-20220519172509224.png)\n\n**还有什么变动会影响DCT呢？**比较简单的是单纯把波整体上下平移，**==看起来升降信号只影响系数X0==**。当提高频率时，发现有多个不同的`DCT`系数与余弦波相对应，当变为`cos(2x)`后只有系数X2不为0，这个余弦波只是将之前哪个余弦波的频率翻倍了。频率为0的余弦波是一个常值信号提供了标准，去衡量**一组像素的整体亮度**\n\n![image-20220519202319099](imagecompress/image-20220519202319099.png)\n\n每个频率都对应一个不同的图像模式，`DCT`的核心是分解出各个基本模式，分析它们对原始图像的贡献，结果发现八个像素的所有可能组合都可表示为这八个余弦波的总和\n\n![image-20220519202612699](imagecompress/image-20220519202612699.png)\n\n`DCT`的数学定义如上，其中`k`为余弦波的频率，原始信号点可以表示为一个矢量，我们可以将余弦波的采样点也改写成矢量\n\n![image-20220519202933577](imagecompress/image-20220519202933577.png)\n\n向量点积的形式是衡量两者相似性的好方法，这解释了当我们对频率为`k`的余弦波采样，将其输入`DCT`后位于`k`号位的系数为何会出现尖峰。这两个向量互为倍数，所以点积被最大化了\n\n![image-20220519204120404](imagecompress/image-20220519204120404.png)\n\n我们可以把整个`DCT`看成是矩阵与向量之积，矩阵每行是各自频率的余弦波中的采样点，**==令人震惊的是矩阵中每行都是相互正交的==**，这就是为什么直接只输入一个特定频率的余弦波时，其他系数没有任何贡献（来自不同余弦波的采样点相互正交 ）\n\n> 扩展：DFT中扩展序列时，是直接采用平移方式引入了不连续的区间，而DCT采用对称形式，消除了这一人为的不连续（扩展后的函数在该点变化非常快，信号分解过程中会出现多个高频信号），而通常高频信号隐藏在这人造的不连续中。这也正是为什么DCT的能量聚集效果会更好。\n\n### JPEG如何具体使用DCT\n\n![image-20220519205438515](imagecompress/image-20220519205438515.png)\n\n首先将图片分为多个8*8的区域，各个元素减去128使值域中心为0，再对这个矩阵的行列进行`DCT`变换，各系数分别代表某个8 * 8图案的权重，其他格子都是第一行和第一列这些基础图案的组合\n\n![image-20220519221457268](imagecompress/image-20220519221457268.png)\n\n只加入小部分的频率，我们的信号和图像也和原图相差无几了\n\n![image-20220519221911043](imagecompress/image-20220519221911043.png)\n\n最大系数在左上角也就是低频部分，有趣的是图里的各个8*8矩阵基本都满足这个性质，这个性质称为能量集中（图像压缩领域的重要概念）。进行变化后，其中最大的那些值会集中在几个低频系数上，复原图像时每个小块只用一部分的系数（**最低频的分量**）肉眼就很难分辨了，正是这个概念让我们在不降低观感的同时能够高度压缩图片。随着频率的提高，人对高频系数逐渐不敏感，就可以丢弃`DCT`的高频分量，如何做呢？\n\n### DCT如何丢弃高频分量\n\n![image-20220520093829648](imagecompress/image-20220520093829648.png)\n\n**==量化：==**给定一个8*8矩阵代表`DCT`输出的频率系数，将每个元素分别除以一个标量值（有不同的量化表分别用于亮度通道和色彩通道），然后取整。\n\n![image-20220520094009730](imagecompress/image-20220520094009730.png)\n\n解码阶段不太重要的高频分量会出现一大堆0，完成量化后，可以利用冗余来进一步压缩，这涉及到`游程编码和霍夫曼编码的组合`。\n\n![image-20220520095419761](imagecompress/image-20220520095419761.png)\n\n编码将系数按之字形排列，让序列尽量多出现连续0，然后用游程编码压缩，霍夫曼编码较复杂此处省略具体操作。\n\n## 总结\n\nJPEG中的许多革新时源于人类视觉系统的实验和理解，这让我们知道人眼对色彩和高频信息并不敏感，音频视频压缩上也可同样的技术应用。\n\n> [视频：JPEG不可思议的压缩率](https://www.bilibili.com/video/BV1iv4y1N7sq?spm_id_from=444.41.list.card_archive.click)\n","tags":["image processing"],"categories":["course"]},{"title":"Swin","url":"/2022/05/17/swin/","content":"\n> 对`霹雳吧啦Wz`swin transformer的代码总结\n\n# 代码结构\n\n`Swin Transformer` ------`PatchEmbed`\n\n​\t\t   \t\t\t\t\t\t\t\t`PatchMerging`                                    \n\n​\t\t\t\t\t\t\t\t\t\t\t`BasicLayer` ------`create_mask`------`window_partition and window-reverse`\n\n​                                            `Swin Transformer Block` ------`Window attention`\n\n## 1. patchEmbed\n\n```python\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D Image to Patch Embedding\n    \"\"\"\n    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        patch_size = (patch_size, patch_size)\n        self.patch_size = patch_size\n        self.in_chans = in_c\n        self.embed_dim = embed_dim\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        _, _, H, W = x.shape\n\n        # padding\n        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding\n        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n        if pad_input:\n            # to pad the last 3 dimensions,\n            # (W_left, W_right, H_top,H_bottom, C_front, C_back)\n            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n                          0, self.patch_size[0] - H % self.patch_size[0],\n                          0, 0))\n\n        # 下采样patch_size倍\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        # flatten: [B, C, H, W] -> [B, C, HW]\n        # transpose: [B, C, HW] -> [B, HW, C]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n```\n\n## 2. PatchMerging\n\n```python\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    # 传入前面保存好的H,W\n    def forward(self, x, H, W):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        x = x.view(B, H, W, C)\n\n        # padding\n        # 如果输入feature map的H，W不是2的整数倍，需要进行padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            # to pad the last 3 dimensions, starting from the last dimension and moving forward.\n            # (C_front, C_back, W_left, W_right, H_top, H_bottom)\n            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同\n            # 此处与1中pad方式不同？\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]\n        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]\n\n        x = self.norm(x)\n        x = self.reduction(x)  # [B, H/2*W/2, 2*C]\n        return x\n```\n\n## 3. BasicLayer\n\n```python\n#---------------SwinTransformer类下构建了BasicLayer-------------------------\n# build layers\nself.layers = nn.ModuleList()\nfor i_layer in range(self.num_layers):\n    # 注意这里构建的stage和论文图中有些差异\n    # 这里的stage不包含该stage的patch_merging层，包含的是下个stage的，构建前三个stage要下采样\n    layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                        depth=depths[i_layer],\n                        num_heads=num_heads[i_layer],\n                        window_size=window_size,\n                        mlp_ratio=self.mlp_ratio,\n                        qkv_bias=qkv_bias,\n                        drop=drop_rate,\n                        attn_drop=attn_drop_rate,\n                        drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                        norm_layer=norm_layer,\n                        downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                        use_checkpoint=use_checkpoint)\n    self.layers.append(layers)\n    \n# 整体前向过程\n    def forward(self, x):\n        # x: [B, L, C]\n        x, H, W = self.patch_embed(x)\n        x = self.pos_drop(x)\n\t\t\n        # 每个Stage中都将当前的H，W传入，并保存经过当前stage后的H,W\n        for layer in self.layers:\n            x, H, W = layer(x, H, W)\n\n        x = self.norm(x)  # [B, L, C]\n        x = self.avgpool(x.transpose(1, 2))  # [B, C, 1]\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n```\n\n```python\n#---------------------------BasicLayer类中构建每个stage---------------------------\n# build blocks\nself.blocks = nn.ModuleList([\n    SwinTransformerBlock(\n        dim=dim,\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0 if (i % 2 == 0) else self.shift_size,\n        mlp_ratio=mlp_ratio,\n        qkv_bias=qkv_bias,\n        drop=drop,\n        attn_drop=attn_drop,\n        drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n        norm_layer=norm_layer)\n    for i in range(depth)])\n\n    def create_mask(self, x, H, W):\n        # calculate attention mask for SW-MSA\n        # 保证Hp和Wp是window_size的整数倍\n        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n        # 拥有和feature map一样的通道排列顺序，方便后续window_partition\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]\n        h_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        w_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]\n        # 每一个窗口中的值展平后复制9次，与所有值复制9次后做差，相邻区域填0\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1] \n        # [nW, Mh*Mw, Mh*Mw]\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        return attn_mask\n    \n#该类的前传过程，每个stage后都会返回H,W\n    def forward(self, x, H, W):\n        attn_mask = self.create_mask(x, H, W)  # [nW, Mh*Mw, Mh*Mw]\n        for blk in self.blocks:\n            # 添加H,W类属性\n            blk.H, blk.W = H, W\n            if not torch.jit.is_scripting() and self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, attn_mask)\n            else:\n                x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x = self.downsample(x, H, W)\n            # +1：防止为奇数后续操作需要填充\n            H, W = (H + 1) // 2, (W + 1) // 2\n\n        return x, H, W\n```\n\n```python\n#-------------------------------- SwinTransformerBlock的具体实现-------------------------------\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,\n            attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, attn_mask):\n        # 在遍历self.blocks的过程中会将H,W复制给类变量，所以此处可以取到H,W\n        H, W = self.H, self.W\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # pad feature maps to multiples of window size\n        # 把feature map给pad到window size的整数倍\n        pad_l = pad_t = 0\n        pad_r = (self.window_size - W % self.window_size) % self.window_size\n        pad_b = (self.window_size - H % self.window_size) % self.window_size\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n        _, Hp, Wp, _ = x.shape\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n            attn_mask = None\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # [nW*B, Mh, Mw, C]\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [nW*B, Mh*Mw, C]\n\n        # W-MSA/SW-MSA,传入了填充后的H,W\n        attn_windows = self.attn(x_windows, Hp, Wp, mask=attn_mask)  # [nW*B, Mh*Mw, C]\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)  # [nW*B, Mh, Mw, C]\n        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # [B, H', W', C]\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n\n        if pad_r > 0 or pad_b > 0:\n            # 把前面pad的数据移除掉\n            x = x[:, :H, :W, :].contiguous()\n\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n```\n\n```PYTHON\n# 包含相对位置编码部分\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # [Mh, Mw]\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # [2, Mh, Mw]\n        coords_flatten = torch.flatten(coords, 1)  # [2, Mh*Mw]\n        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, Mh*Mw, Mh*Mw]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # [Mh*Mw, Mh*Mw, 2]\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # [Mh*Mw, Mh*Mw]\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, Mh*Mw, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        # [batch_size*num_windows, Mh*Mw, total_embed_dim]\n        B_, N, C = x.shape\n        # qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]\n        # reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]\n        # permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        # transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]\n        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        # relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Mh*Mw, Mh*Mw]\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            # mask: [nW, Mh*Mw, Mh*Mw]\n            nW = mask.shape[0]  # num_windows\n            # attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]\n            # mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        # transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]\n        # reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n```\n\nLINE247 `self.attn(x_windows, H, W, mask=attn_mask)`加入了H,W\n\nLINE57 新增卷积分支\n\nLINE81 `forward`中传入H,W\n\nLINE89-105 更新V\n\n","tags":["code"],"categories":["source code"]},{"title":"A White-box Deep Network from the Principle of Maximizing Rate Reduction","url":"/2022/05/15/A White-box Deep Network/","content":"\n> What I cannot create, I do not understand. - Richard Feynman\n\n`马毅介绍了近期的工作：通过优化 MCR^2 目标，能够直接构造出一种与常用神经网络架构相似的白盒深度模型，其中包括矩阵参数、非线性层、归一化与残差连接，甚至在引入「群不变性」后，可以直接推导出多通道卷积的结构。该网络的计算具有精确直观的解释，受到广泛关注。这个框架不仅为理解和解释现代深度网络提供了新的视角，有可能地改变和改进深度网络的实践所得到的网络将完全是一个“白盒”，而随机初始化的反向传播不再是训练网络的唯一选择。`\n\n# 0. High-Dim Data\n\n![image-20220515135836762](paper/image-20220515135836762.png)\n\n每一类的数据一定在高维空间中的一个低维的流形或者分布上，实际上整个机器学习就是在学到底谁是谁，找到这些结构。所有目前基于数据的AI或者机器学习都在做以上三件事情：\n\n> 1. 数据插值——寻找样本之间的相似关系，这体现为聚类或分类任务；\n> 2. 在上一步表现良好的基础上，可以判断新的数据所属类别\n> 3. 当对以上两点理解不错的时候，可以考虑将数据表达的更好\n\n![image-20220515141128956](paper/image-20220515141128956.png)\n\n整个网络学习过程中我们并不知道数据内在的结构是什么\n\n![image-20220515141330628](paper/image-20220515141330628.png)\n\n很多理论希望去解释深度学习到底在干什么，其中一种是说：深度网络其实是从数据中抓取一些与标签最相关的特征，同时把不相干的特征扔掉，与具体任务强相关不通用。\n\n![image-20220515142430195](paper/image-20220515142430195.png)\n\n> 1. 从计算的角度来说，在处理高维数据的时候，传统信息论的统计量是无法定义的，高维数据常常是退化分布的，无法完成有效测量。\n> 2. 当数据有低维结构时，这些量在数学上甚至没有定义\n>\n\n很多NIPS文章提出一个理论，紧接着马上开始近似，某些量是independence开始计算，两三步后与原始信息的区别和联系都不知道了，如何提供指导。\n\n##  1. 区分数据\n\n![image-20220515151501291](paper/image-20220515151501291.png)\n\n![image-20220515144328888](paper/image-20220515144328888.png)\n\n传统聚类方法通常采用最大化相似度的方法进行，而应用在高维退化分布的数据上时，相似度难以定义。因此，我们从更基础的问题出发，为什么需要聚类划分数据？\n\n从压缩角度，我们可以看出，能够划分的数据具有更小的空间，通过划分能够获得对数据更有效的表示。如果能找到编码长度的有效度量，就可以设计相应的优化目标。\n\n熵是度量编码长度的工具，但在高维数据上，熵的测量非常困难，马毅教授采用率失真理论来度量这样的表示，提出了编码长度函数（Coding Length Function）\n\n![image-20220515144801577](paper/image-20220515144801577.png)\n\n上述公式相当于给一组数据就输出其需要的bits存储空间\n\n有上图的度量后，我们就能描述聚类或划分的现象，即划分前的数据所须的编码长度，大于划分后的编码长度。这样的划分不需要标签，而是可以通过一些贪心算法，比较不同划分之间的编码长度，获得使划分后编码长度最小的划分。结果展现了这样的方法有非常好的聚类效果，能够找到全局最优的划分，并对离群点非常鲁棒。\n\n![image-20220515145951032](paper/image-20220515145951032.png)\n\n整个的数据分类就变成一个压缩的问题，要分开的话，其分开后的压缩的量用的bits最少\n\n![image-20220515150154121](paper/image-20220515150154121.png)\n\n这个量非常神奇，你可以把数据每个点都分开，再两两融合（如果可以省bits），可以得到一个`pwm`算法 \n\n![image-20220515150847050](paper/image-20220515150847050.png)\n\n神奇在数据有很多的噪声和异常值的情况下，都能找到低维结构\n\n![image-20220515151029152](paper/image-20220515151029152.png)\n\n仅使用数据压缩的算法，就取得了当时最好的分割效果\n\n## 2. 划分新数据\n\n![image-20220515151407157](paper/image-20220515151407157.png)\n\n当数据有低维结构时此公式不适用，教的是上图公式做法，但实际应用中并不用统计教科书中的方式。\n\n![image-20220515151907078](paper/image-20220515151907078.png)\n\n不适用应该怎么办呢最大似然估计不work，又回到编码量。如果要对某个样本分类，那么包含其后那一类相比其他类的编码量应该是最小的。\n\n同样的方法可以应用于分类任务，通过比较将新数据划分到不同类别增加的编码长度，选取使编码长度增加最少的类别，作为该样本最合适的分类，这种方法依旧来源于最小划分后编码长度的理论。这种方法可以理解为，将新样本划分到合适的类别分类后，所带来的存储开销应当最少，通过正确分类，可以得到最优的表示效率。结果显示，比较传统方法，MICL能够找到更加紧的边界，并且与分类不同的是，其决策边界更接近于数据本身的结构特征。\n\n![image-20220515152725594](paper/image-20220515152725594.png)\n\n整个过程可以通过数学证明\n\n![image-20220515152934139](paper/image-20220515152934139.png)\n\n通过简单的分类，已经学到了数据的低维结构，但只是对低维子空间和高斯分布很准，当数据有非线性结构不适用。\n\n##  3. 表征数据\n\n`在完成了 Interpolation(聚类)与 Extrapolation(分类)后，从压缩的视角，还能够实现对数据的表示。当数据符合某种低秩结构时，优秀的表达的目标可以被理解为，最大限度地学习到该结构特征，即，在让同一结构样本靠近的同时，使样本表达能力最大；同时，将不同结构数据间的差异尽可能清晰地体现出来。`\n\n![image-20220515153419946](paper/image-20220515153419946.png)\n\n![image-20220515153857958](paper/image-20220515153857958.png)\n\n![image-20220515154142217](paper/image-20220515154142217.png)\n\n如何判断变换后特征的好坏呢？通过衡量整体和局部平均编码量来衡量\n\n![image-20220515154630286](paper/image-20220515154630286.png)\n\n![image-20220515154901992](paper/image-20220515154901992.png)\n\n同类（局部）越近越好，不同类（整体）越远越好\n\n![image-20220515160633316](paper/image-20220515160633316.png)\n\n左优于右，蓝色球数量多。为了使不同范围的样本进行比较，针对每个样本需要进行归一化操作。这与归一化的通常理解相符，使模型能够比较不同范围的样本。\n\n![image-20220515194314867](paper/image-20220515194314867.png)\n\n在宽泛的条件下，数学上可以证明当特征达到最大编码量的时候，可以证明每类的特征，彼此是正交的；每类的特征可以把子空间占满。\n\n## 4. 实验（MCR取代Cross Entropy）\n\n![image-20220515195017779](paper/image-20220515195017779.png)\n\n训练过程中的三个量是在物理、几何和统计上意义严格的量，整体的Vol在增长，局部的Vol在压缩，差值在增长。\n\n![image-20220515195514951](paper/image-20220515195514951.png)\n\n不同类的特征完全正交，每一类的特征均匀的分布在各个子空间上。\n\n![image-20220515195843123](paper/image-20220515195843123.png)\n\n目标函数不再拟合label，从整体去学习特征，鲁棒性很强\n\n## 5. 本质是什么\n\n![image-20220515200105407](paper/image-20220515200105407.png)\n\n![image-20220515200746850](paper/image-20220515200746850.png)\n\n这是个非凸的问题，我们的目的是研究什么样的Z能优化这个问题，不管是deep learning或是其他方式。神经网络告诉我们当遇到一个很难的问题时，其他做不了可以做梯度下降，训练一下调一调。\n\n![image-20220515201237687](paper/image-20220515201237687.png)\n\n对该目标求梯度后，获得了两个操作矩阵E、C，所求梯度就是其分别与样本乘积的和。\n\n![image-20220515201743335](paper/image-20220515201743335.png)\n\n而观察E、C两个操作矩阵，会发现其与样本乘积的结果天然带有几何的解释，即矩阵作用于所有的样本数据上，和其他的矩阵作用于不同类的数据上。当使用其他的数据对每个数据做回归得到的残差，就是\n\n因此，若需要扩展样本空间的大小，只需加上E与样本相乘获得的残差，若要压缩各类别子空间的大小，仅需减去与C进行相同操作的结果。\n\n![image-20220515204644473](paper/image-20220515204644473.png)\n\n![image-20220515204808657](paper/image-20220515204808657.png)\n\n每个EC、CZ都是自回归的残差。对比常 用的神经网络结构，可以发现其与`ReduNet`有许多相似之处，例如残差链接，C的多通道性质，非线性层等。同时，`ReduNet`所有参数均能够在前向传播中计算得到，因此网络无需BP优化。\n\n![image-20220516144746606](paper/image-20220516144746606.png)\n\n通过引入组不变性，将cyclic shift后的样本视为同一组，每次将一组样本编码到不同低秩空间，`ReduNet`可以实现识别的平移不变性。同时，类似卷积的网络性质也随之而来。在引入平移不变的任务要求后，网络使用循环矩阵表示样本，因而在与E，C矩阵进行矩阵乘时，网络的操作自然地等价于循环卷积。在压缩数据的过程中，梯度下降中的算子自动变成卷积。\n\n![image-20220516150617561](paper/image-20220516150617561.png)\n\n求逆计算使得通道间的操作相互关联。上述计算还可以通过频域变换来加速计算效率。\n\n![image-20220516151156033](paper/image-20220516151156033.png)\n\n整个优化过程中自然的出现了神经网络中的提出的种种算子。尽管上文中算法有诸多变化，其核心都是基于“压缩”的概念。 聚类，划分，表征，这些学习任务都可以被表述成压缩任务。我们希望学习到样本的知识，是期望能够更高效地表示样本，因此我们学习类别，提取特征，抽象概念。 MCR^2 原理基于率失真理论，描述了划分和压缩的过程，并能够基于压缩，完成包括聚类，分类，表示学习，构造网络等等任务，体现了作为学习的一般原理的泛用性能。\n\n![image-20220516153251893](paper/image-20220516153251893.png)\n\n**QA:** \n\n每个神经元可能是原始高维空间中的一个切割平面 ，新来的数据和切割平面去比到底在切割平面的哪一边，一个神经元的输出可以得出属于哪一边，相当于encoding；可以在这个高维空间中一直切，切到一定的程度，对于任何input都可以encoding，可以基本知道这个点在高维空间中的位置。神经元的数学模型可能就是高维空间中的切割平面，用来做encoding，每一层的个数相当于数据的内在结构的维度。第二层的神经元相当于使近的更近，远的更远，后续层相当于优化迭代的次数。网络是前向学习的，即使只有少量样本就能学到很好的参数，学习过程中不用大量的资源。\n\n![image-20220516161208731](paper/image-20220516161208731.png)\n\n> - [其他总结](https://www.cn-healthcare.com/articlewm/20210428/content-1214792.html)\n>\n> - [视频讲座](https://www.bilibili.com/video/BV1LB4y1u79M?spm_id_from=333.788.b_636f6d6d656e74.5)\n","tags":["paper"]},{"title":"Pytorch","url":"/2022/04/25/pytorch/","content":"\n# Module类  \n\n- module中的__getattr中有三个魔法函数，其有三个成员变量分别是`_parameters`、`_buffers`(统计量)和`_modules`\n\n```python\n# 实例化模型后，参数已初始化\ntest_module = Net()\n# 返回模型中各个子模块层,返回的是有序字典，与named_children()区别，其返回的是元组\ntest_module._modules\n# 打印模块自身的Parameters、buff对象，不包括其中各个子模块\ntest_module._parameters\ntest_module._buffers\n\n# named_modules()不仅返回自身模块还返回各个子模块 \nfor p in test_module.named_modules():\n    print(p)\n```\n\n- module中的`state_dict`方法\n  1. 首先，通过`_save_to_state_dict`方法先将当前模块的`parameters`和`buffers`变量存入`destination`字典\n  2. 然后，遍历`self.modules.items()`子模块，将每个子模块的`parameters`和`buffers`变量存入`destination`字典\n  3. 最后返回`destination`字典中包含所有模型状态参数，OrderedDict[key, value]\n\n```python\n>>> module.state_dict().keys()\n['bias', 'weight']\n\n# 将网络参数保存到path文件中，不包含网络图结构和优化器参数等部分\ntorch.save(test_module.state_dict(), path)\n\n#———————————————————————————模型保存相关—————————————————————\n# 保存网络参数和图结构，占用磁盘空间大\ntorch.save(test_module, path)\n\n# 保存所有所有相关信息\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\ntorch.save({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)\n\n# 再次实例化，是因为上面torch.save中没有保存网络图结构，要先构建图结构再去加载参数\nmodel = Net()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = torch.load(PATH)\n# 传入字典对象\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n```\n\n- module中的`parameter`方法\n\n  1. 注意`parameter` 与`_parameters `区分，前者返回的是迭代器，包括当前模块和各个子模块的参数。后者返回的是当前模块中的参数。\n  2. 其中会执行`named_papameters` 最终返回的是个迭代器对象[key, value]\n\n  ```python\n  >>> for p in test_module.named_parameters():\n      \tprint(p)\n  ```\n\n- module中的`train` 方法\n\n  `dropout` 和 `batchnorm` 都继承了Module类，都属于模型的子模块，当把模型设置为训练模式和验证模式时，相应的子模块也会设为对应的训练或验证模式\n\n# 自动微分Forward与Reverse模式\n\n- ## Forward计算流程\n\n![forward](pytorch/image-20220426135242639.png)\n\n**==特点：==** 前向计算过程中当前节点相对某个输入结点的梯度可计算得到；每次只能得到一个输入节点的导数如图中的x1\n\n- ## Reverse计算流程\n\n![reverse](pytorch/image-20220426135907575.png)\n\n**==特点：==** 反向计算过程中需要等待前向计算结束；一次性可以算出所有节点导数 \n\n> 图片出处：Automatic Differentiation in Machine Learning: a Survey\n\n# 算子融合\n\n```python\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nin_channels = 2\nout_channels =2\nkernel_size = 3\nw = 9\nh = 9\n\nx = torch.ones(1, in_channels, h, w)\nconv3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconv1 = nn.Conv2d(in_channels, out_channels, 1)    # [2,2,1,1]\nresult1 = x + conv3(x) + conv1(x)\nprint(result1)\n\n# ——————————————————————--————将1*1卷积转为3*3——————————————————————————————————————\n\n# pad填充方式从里到外，每个维度都有上下和左右两个方向，四个1分别对应左右S上下填充0的个数\n# [2,2,1,1] -> [2,2,3,3]\nconv1_to_conv3 = F.pad(conv1.weight, [1,1,1,1,0,0,0,0]) \n\n# 实例化卷积\nconv1_3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\n\n# 将卷积参数用1*1填充后的参数替代，weight是parameter类，要用nn.Parameter()包装\nconv1_3.weight = nn.Parameter(conv1_to_conv3)\nconv1_3.bias = conv1.bias\n\n#--------------------------------如何将输入x本身化为3*3卷积表示—————————————————————————————————\n#------------------------1. 必须是1*1的卷积不考虑相邻点融合--------------—————————————————————\n#-------------------—2. 不考虑通道间的融合（只有一个通道中含有非0数）----------------------------\n\n# zeros:不考虑通道之间影响\tchannel:不考虑相邻点影响  \tweight[2,2,3,3]：共四个3*3矩阵\nzeros = torch.unsqueeze(torch.zeros(kernel_size, kernel_size), 0)\nchannels = torch.unsqueeze(F.pad(torch.ones(1, 1), [1,1,1,1]), 0)\n\n# 对应第一个通道卷积核\nchannel_zeros = torch.unsqueeze(torch.cat([channels, zeros], 0), 0)\n\n# 对应第二个通道卷积核\nzeros_ channel= torch.unsqueeze(torch.cat([zeros, channels], 0), 0)\n\nidentity_conv_weight = torch.cat([channel_zeros, zeros_ channel], 0)\nidentity_conv_bias = torch.zeros([out_channels])\n\nconvx_3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconvx_3.weight = nn.Parameter(identity_conv_weight)\nconvx_3.bias = nn.Parameter(identity_conv_bias)\n\nresult2 = conv3(x) + conv1_3 + convx_3\nprint(result2)\n\n#--------------------------------融合—————————————————————————————————\nconv_fusion = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconv_fusion.weight = nn.Parameter(conv3.weight.data + conv1_3.weight.data + convx_3.weight.data)\nconv_fusion.bias = nn.Parameter(conv3.bias.data + conv1_3.bias.data + convx_3.bias.data)\nresult3 = conv_fusion(x)\nprint(torch.all(torch.isclose(result2, result3)))\n```\n\n# Hooks机制\n\n> Pytorch提供的hooks机制能让用户可以往计算流中的某些部分注入代码，一般来说这些部分无法直接从外部访问。其中主要有两种hooks，一种是添加到张量上的hooks，另一种是添加到Module上的hooks。\n\n## 添加到张量上的hooks\n\n这些添加的`hooks`能够让用户在反向传播的过程中访问到计算图中的梯度。\n\n下面先来看看反向传播的一个具体例子。\n\n![image-20220523195647470](pytorch/image-20220523195647470.png)\n\n- 当我们将张量`a`和`b`相乘的同时也在构建后向图，即创建了一个名字是`MulBackward0`的节点(其中`next_functions`表示梯度接下来要传递的到哪些节点即操作的输入)，还有两个`AccumulateGrad`节点(将反向传播过程中对应张量的梯度作累加)。最后得到的梯度值将保存到叶子张量上(绿框)。\n\n- `张量c`属于中间节点，其中`grad_fn`属性指向后向图中的`MulBackward0`节点，`c.backward()`就是对应这个过程将起始梯度传给`MulBackward0`节点。然后再将该梯度传给`MulBackward0`节点中的`backward`函数(本质就是将输入梯度乘对应值得到输入张量的梯度)。本例中前传`a*3`和`2*b`所以此节点对应张量`a`和`b`的梯度为3，2。\n\n  最后要将输出梯度1分别乘3，2得到输入梯度，然后传递给`a`和`b`的`AccumulateGrad`节点来累加梯度，该节点将最终的梯度赋值给对应张量的`grad`属性。\n\n以上整个过程一旦调用了`.backward()`反向传播过程中中间节点所产生的梯度(红框)都是无法访问到的，无法打印、修改，用户只能查看反向回传完之后叶子节点上的梯度。\n\n**hooks的作用在于能够让用户访问到反向传播过程中的梯度张量，同时可以修改这些梯度值。**\n\n------\n\n我们给中间的张量都添加`hooks`看看计算图会有什么变化\n\n![image-20220523204413424](pytorch/image-20220523204413424.png)\n\n- 第一个添加的`hook`: `c.register_hook`中传入了一个函数`c_hook`,该函数有一个参数表示梯度，并可以返回一个新梯度。当向张量`c`注册这个`hook`函数，首先它会被添加到张量`c`的`_backward_hooks`（是个有序字典，添加`hook`函数的顺序很重要，反向传播中会按照之前添加的顺序调用）。\n\n- 如果用户想让梯度保存在某个中间节点的话，需要调用`中间节点的retain_grad函数`(默认情况下，只有叶子节点会保存梯度值)。在调用此函数后，会往`_backward_hooks`字典中注册`retain_grad_hook`函数，当该函数被调用，传给它的梯度值会保存到中间张量的`grad`属性上。\n\n- **需要注意的是反向传播过程中`hook`系统是如何工作的，往中间节点和叶子节点上添加`hook`是有区别的。**当往叶子节点添加`hook`函数，该函数就只是被添加到`_backward_hooks`字典中。而往中间节点添加`hook`函数的同时，所有在反向图中关联了该中间向量的节点都会被通知，上图中`MulBackward0`节点关联了张量`c`，即将`_backward_hooks`字典添加到`MulBackward0`节点的`pre_hooks`列表中，这些列表中函数都会在梯度被传递给`backward`函数前被调用。\n\n  在注册好所有hook函数后，过一遍反向传播过程。\n\n![image-20220523220453700](pytorch/image-20220523220453700.png)\n\n流程：`1.0`->`MulBackward0`->`pre_hooks`->`_backward_hooks`->将梯度2->`backward`->8,12(12会被传递给叶子张量d的`AccumulateGrad`节点，**同时该节点会检查其所关联的张量是否有注册**`backward_hooks`，如果注册了（`d`中注册了，`a,b`未注册），该节点会把梯度传给这些注册的`hook`函数处理，然后再保存到张量的`grad`属性上)->梯度10->`backward`-> ....\n\n![image-20220523221008946](pytorch/image-20220523221008946.png)\n\n`h.remove()`可以将`hook`函数从保存它的`_backward_hooks`字典中移除，如上图在调用`e.backward`之前调用了`h.remove`，那么在反向传播中这个`c_hook`函数就不会被调用，另外要注意的是，在这些`hooks`函数中不要对梯度张量本身做任何修改，即不要对输入梯度做`inplace`操作如`grad *= 100`。原因是这个梯度有可能同时被传递给后向图中其他节点。\n\n##  Module上的hooks\n\n- `Module`上的`hooks`函数是在`forward`函数调用之前或之后被调用的。下面例子实现了一个`SumNet`模块，其`forward`函数是将三个张量相加并返回结果。\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass SumNet(nn.Module):\n    def __init__(self):\n        super(SumNet，self)._.init__()\n        \n    @staticmethod\n    def forward(a，b，c):\n        d = a + b + c\n        return d\n\ndef forward_pre_hook(module，inputs):\n    a，b = inputs\n    return a + 10，b\n\n#此处inputs参数是forward_pre_hook函数的返回值input(11,2),output的输出会覆盖forward函数中返回的输出，即116\ndef forward_hook(module，inputs， output):\n    return output + 100\n\ndef main():\n    sum_net = SumNet()\n    \n    # 往模块注册在forward之前调用的hook函数，执行完该函数后，会将更新后的a=11, b=2, c=3输入forward函数，并返回d=16\n    sum_net.register_forward_pre_hook(forward_pre_hook)\n    #注册在forward之后调用的hook函数\n    sum_net.register_forward_hook( forward_hook)\n    \n    a = torch.tensor(1.0，requires_grad=True)\n    b = torch.tensor(2.0，requires_grad=True)\n    c = torch.tensor(3.0，requires_grad=True)\n    \n    #a, b作为位置参数传入，c作为 关键字参数传入\n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#116\n```\n\n- 和往张量上注册`hooks`函数一样，同样可以用一个变量来保存注册`hooks`函数时的返回值，即`hook`函数的句柄（handle to the hook），这样方便后面移除`hook`。\n\n```python\ndef main():\n    sum_net = SumNet()\n    \n\tforward_pre_hook_handle = sum_net.register_forward_pre_hook (forward_pre_hook)\n\tforward_hook_handle = sum_net.register_forward_hook(forward_hook)\n\t\n    a = torch.tensor(1.0，requires_grad=True)\n    b = torch.tensor(2.0，requires_grad=True)\n    c = torch.tensor(3.0，requires_grad=True)\n    \n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#116\n    \n\tforward_pre_hook_handle.remove()\n\tforward_hook_handle.remove()\n    \n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#6\n```\n\n- `Module`还有另一种`hook`，即`backward_hook`。`register_backward_hook(backward_hook)`\n\n  ```\n  # args：实例、输入梯度、输出梯度\n  def backward_hook(module，grad_input，grad_output):\n  \tprint('module:', module)\n  \tprint('grad_input:',grad_input)\n  \tprint('grad_output:',grad_output)\n  ```\n\n  > [PyTorch Hooks Explained - In-depth Tutorial](https://www.bilibili.com/video/BV1MV411t7td?spm_id_from=333.337.search-card.all.click)\n\n","tags":["torch"],"categories":["source code"]},{"title":"Convnext","url":"/2022/03/30/convnext/","content":"\n# 网络结构\n\n## Block\n\n<img src=\"convnext/block.png\" alt=\"convnext\" style=\"zoom:33%;\" />\n\n``` python\nclass Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n    Args:\n        dim (int): Number of input channels.\n        drop_rate (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6, data_format=\"channels_last\")\n        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        # 对应Layer Scale\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_rate) if drop_rate > 0. else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            # 对每个通道数据缩放\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]\n\n        x = shortcut + self.drop_path(x)\n        return x\n```\n\n# 整体结构\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:6%\"><img src=\"convnext/network.png\"></div>\n        <div style=\"display:table-cell; width:20%;\"><img src=\"convnext/config.png\"></div>\n    </div>\n</div>\n\n\n```python\nclass ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n    def __init__(self, in_chans: int = 3, num_classes: int = 1000, depths: list = None,\n                 dims: list = None, drop_path_rate: float = 0., layer_scale_init_value: float = 1e-6,\n                 head_init_scale: float = 1.):\n        super().__init__()\n        # stem and 3 intermediate downsampling conv layers\n        self.downsample_layers = nn.ModuleList()  \n        # dims[0]对应每个stage输入特征通道数\n        stem = nn.Sequential(nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n                             LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"))\n        self.downsample_layers.append(stem)\n        \n        # 对应stage2-stage4前的3个downsample，downsample：LN+Conv2d k2,s2\n        for i in range(3):\n            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                                             nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2))\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple blocks\n        # depth对应上图B中参数\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0 #计数，当前blcok之前已经构造好的block个数\n        # 构建每个stage中堆叠的block，j在当前block中构建第几个blcok\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)\n                  for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n            \n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.trunc_normal_(m.weight, std=0.2)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n\n        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n    \ndef convnext_tiny(num_classes: int):\n# https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\nmodel = ConvNeXt(depths=[3, 3, 9, 3],\n                 dims=[96, 192, 384, 768],\n                 num_classes=num_classes)\n    return model\n```\n\n## v5对应的配置参数\n\n```markd\n# YOLOv5 backbone\n# [from, number, module, args]\n# 3：初始通道数，后续并没有用到\nbackbone:\n  [[-1, 1, ConvNeXt_Block, [96, 0, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [192, 1, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [384, 2, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [768, 3, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n  ]\n```\n\n\n\n\n\n\n\n\n\n\n\n​    \n\n\n\n\n\n\n\n","tags":["code"],"categories":["paper"]}]