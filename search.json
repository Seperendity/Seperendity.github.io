[{"title":"RMB Exchange Rate and RMB Internationalization","url":"/2023/07/15/RMB Exchange Rate and RMB Internationalization/","content":"\n# 一、对人民币汇率的错误预测\n\n基准利率就是一个市场上利率水平的风向标。各个金融机构，都根据这个标准来确定自己的贷款和存款利率。央行提高或者降低基准利率，就意味着提高或者降低整个社会的利率水平。中国经济增速快，美欧日相对较慢，所以中国基准利率较昂贵，海外的无风险收益率较低 \n\n2014-2016流行观点\n\n经济增速下降，看空人民币汇率长期前景；房地产泡沫不可持续，远被高估未来会回到基本面，要么房价跌、要么汇率跌；地方政府债务风险大；产业资本对外转移；M2与GDP的悬殊，水分大\n\n![image-20230715143701523](RMB Exchange Rate and RMB Internationalization/image-20230715143701523.png)\n\n## 1.1 经济增速与汇率的关系\n\n顺周期经济体一般正相关，一般发展中国家；逆周期经济体一般负相关，日元美元。**经济增速和汇率不是决定关系**\n\n## 1.2 资产泡沫与汇率的关系\n\n资产价格不具有一价定理，一线城市未来房价不一定会回归均值\n\n## 1.3 债务率与汇率关系\n\n略有相关，表现为债越多汇率越强，有钱的才能借到更多钱，国家也如此。**衡量债务风险的不是债务率而是债务的定价货币**，本币借的债本质上不是债，而是对持有这种货币的人征收的一种隐形的税收，地方政府借的是本币债\n\n## 1.4 资本转移\n\n变为资本输出方后汇率会有更强的稳定性和逆周期性，经济好时，汇率下行，经济差时汇率更强势。（参照日本70-80年代产业转移汇率变化）\n\n## 1.5 M2与汇率\n\n不同国家金融体制不同，美国直接融资非常发达，金融衍生品很发达， **评估美国融资风险，应该用M3最广义的货币供应量**，中国主要靠**银行信贷，用M2** \n\n无法简单放在一起对比， **货币发行增速和汇率之间是明确不相关的** \n\n **支撑人民币汇率强势的内在力量本质上是产业的升级和老龄化** \n\n# 二、长期汇率由什么决定\n\n## 2.1长期影响汇率的因素\n\n一个国家可贸易品的价格水平其决定因素是，技术水平和人口老龄化程度。\n\n## 2.2 影响汇率的六个深层因素\n\n**国家能力**：财政开支占本国GDP开支越大，汇率长期越坚挺；\n\n**贸易开放度：** 经济体中制造业占比高其中**研发支出占比越高**，汇率越强，如日本；\n\n**要素特征：** 人还是物，是脑力还是苦力，对汇率有较强解释性\n\n**文明类型：** 世界上最坚挺的货币有两大类，一是新教文明，欧洲的西部、北部和北美洲、大洋洲的这些经济体，荷兰——英国——德国——美国——北欧\n\n东亚文明：日本——朝鲜半岛——中国大陆——新加坡——越南，不信神，对神持有一种实用主义态度\n\n南亚——东南亚的货币汇率比较“软”，小乘佛教或印度教，不鼓励生产、消费、享乐，追求inner peace，供给需求双向萎缩比较能够忍受贫穷，货币贬值的不是很厉害\n\n三大正常宗教罗马天主教、东正教、伊斯兰教，共同特点都不鼓励生产、追逐尘世的财富，甚至鼓励教徒消费、寻求快乐，如拉美和南欧的天主教文化enjoy life。所以债务产生就是常态，如果债借多了还不起，就会导致汇率大幅贬低。而 **东亚和新教文明鼓励自己的子民追逐世俗的财富和成功，以证明自身价值** 。勤俭从事生产，节约不消费，攒下的剩余变为对他人的股权和债权，这种我们视为美德的在其他三大一神教文明来看是贪婪的表现，是一种罪。\n\n**人均智商和宗教严肃程度：** 平均智商和宗教的虔诚度和严肃程度有明显的负相关，东亚是全球人均智商最高的区域，对宗教采取了非常实用主义态度，像种询盘听说你这有种能力是我需要的，我这可以为你烧高香塑金身，make a deal\n\n# 三、中国被动成为最大外汇储备国\n\n## 3.1 外汇储备源自对汇率波动的恐惧\n\n发展中国家缺乏定价权，利润单薄，汇率波动大，就可能无利可图，所以政府在工业化过程中试图把本币和全球主流的储备货币绑定保持汇率尽量不波动。\n\n## 3.2 中国外汇储备是如何形成的\n\n快速工业化——大量招商引资（92-12）——外资在结售汇制度下向银行兑换人民币——银行外汇头寸管理制度任何机构留存的外汇是有限额的，多出的必须在银行间外汇市场卖出——各地银行都有多余外汇出售，人民币需求大有升值压力——人民银行在外汇市场收购外汇（外汇占款）——国家外汇管理局——投到全球金融资产市场\n\n被动增加了很多基础货币——通胀压力——对冲流动性——央行发行央票、提高存款准备金率（相当于纳税人承担了成本）\n\n## 3.3 外汇拿去干什么了\n\n中国经济体客观上是美元的主要空头力量，大部分美元会被替换为其他的资产池日元、英镑等中获取收益\n\n## 3.4 外汇储备无助于汇率稳定\n\n1.持有巨额外汇储备的代价\n\n巨额的外汇储备——巨量外汇占款和由此而来的基础货币扩张——**央行对冲成本持续积累、资产价格泡沫**\n\n只能被动持有美低收益国债，购买力不断减少，长期持有代价巨大\n\n2.巴西雷亚尔案例\n\n做空者怕的不是外汇储备有多少，而是资本项下的交易管制。\n\n## 3.5 中国不再需要外汇储备\n\n人口结构的改变——已经不再需要巨量的外额储备来维持较低汇率了，以后**人民币长期强势才是要担心的**\n\n# 四、人名币汇率波动的历史\n\n## 4.1 05年之前，汇率为出口导向型工业化提供支持\n\n2005年之前，人民币汇率政策主要是为出口导向型的工业化提供支持。人民币从80年代的`1:1.5`持续贬值到了90年代初的`1:8.7`，一度年化贬值达到了15%。1994-2005年间，汇率长期稳定在`1:8.28`，执行有管理的浮动汇率制度，但实际是固定汇率，十年如一日保持汇率稳定。这期间，97-98金融风暴中，东南亚各国汇率大贬值，中国宣布人民币不搞竞争性贬值。虽然实现了不贬值预期，保持了香港的联系汇率制度，但是一定程度上影响了中国的出口商，直到2001年加入`WTO`才出现转机。\n\n中国在亚洲金融风暴中的负责任表现，为中国在东南亚赢得了负责任大国的形象。2001年911后美国的战略中心转移到中东反恐，同时由于地区大国日本在危机期间搞货币竞争性贬值，使得中国在东南亚的影响力持续提升。2005年，菲律宾总统阿罗约明确表示中国才是地区可以信赖的老大哥。\n\n## 4.2 汇率主动、温和升值是好事吗\n\n中国加入`WTO`后每年的出口增速都在20-30%，积累了大量贸易顺差，2007年贸易顺差占gdp10%。美国开始要求人民币升值。一个议员要求升值15%，另一个要求40%，最后取平均，要求我们升值27.5%。国内同时面临来自央行的升值压力和来自纺织、船舶等出口导向传统行业的贬值压力。\n\n在此内外环境下，2005年7月21日，人民币跳升2%，之后持续渐进升值，一直到08年底升值到了1：7。在此期间我们的汇改原则：自主、渐进、可控，每月千分之4-5，一年升值5-6%。 这种平滑的确定性升值一方面稳定了出口商的汇率预期，但是另一方面由于它的**单边一致性**，吸引了大量的**套利交易**。\n\n例如，同时在香港和深圳开贸易公司。香港借入低息美元，利率3%，通过高价值商品贸易（贵金属）与深圳公司交易，将美元以贸易方式转入国内，存入国内银行获取4%的利息收入。同时获取汇率升值的收益，这样总共可以获得6-7%的年收益。再加上高倍杠杆可以获取暴利。05年后大量热钱涌入，“自我实现”，推升了国内资产泡沫也导致了资本市场6000点大牛市（泡沫）\n\n## 4.3 后金融危机时代，人民币汇率何去何从\n\n08年金融危机之后，人民币短期（-2010年）绑定美元（固定6.8），之后又缓慢升值，但是速度大大放缓，且局部波动性加大，-2014年升值到6.1左右，每年升值2%+。**2014年后出现了一个贬值周期，三年贬值10+%**，-2016年贬值到6.9，每年贬值4+%。当时，人民币贷款利率6%，美元收益率1%。如果贬值太快，做空人民币会有利润，如果不贬值，潜在的贬值压力无法释放。因此贬值4%是一个比较合理的时间换空间的审慎做法。\n\n这轮贬值周期的原因，有两笔钱需要出去。第一，前期升值周期中积累的国际热钱，他们发现贬值苗头后会出逃，而不会留在国内搞建设（另外一点，美元2015年结束了长期的降息周期，开启了加息周期）第二，我们的工业化特点，在92年后，尤其0608年后，中国有着**巨量的FDI（外商直接投资）**，不同于日韩等国，每年有成百上千亿外资因为超国民待遇来中国投资，参与中国的工业化盛宴。\n\n这些外资相当于游牧民族，收割发展中国家的年轻劳动力。中国在2012年后开始提升发展质量，劳动力、环境要素等不再便宜贱卖，这些FDI开始逐步撤离中国。2017年后，又开始了升值，且反弹迅猛，从1:7升到了1:6.26，主要原因是此前太悲观。\n\n2018-19年，贸易战开打，人民币出现了贬值，但是政府这次并没有出来安抚。2020年重新开始升值。面对未来，我们发布了CFETS人民币指数，即人民币同一篮子货币指数，它表明人民币开始主动与美元脱钩，盯住自己的主要贸易一篮子货币，也是人民币换锚的过程。以后的汇率稳定，不是对美元的稳定，而是只对我们主要贸易伙伴的一篮子货币的稳定。（以后人民币将掌握自己的汇率主导权，而变为其它一篮子货币开始紧盯人民币，人民币变成其它货币的锚。）\n\n# 五、铸币税及其国际国内再分配效应\n\n## 5.1 铸币税\n\n货币发行利润或者说是向信任和储存货币的人群征收的税，71年后货币创造成本越来越低\n\n## 5.2 无锚货币时代，不应再痴迷储蓄\n\n货币发行增速以每年20%以上速度复合增长时，勤俭储蓄的人实际扮演了多缴税的角色，这笔隐形税收实际帮助了出口部门剥夺了国内的储蓄者，有很强大的财富再分配效应。用好杠杆做好投资，同理国家也是如此，积累了4万亿的美欧国债，国债收益率远不及央行资产负债表的扩张速度，存在海外的财富其实不断蒸发了\n\n## 5.3 货币扩张对不同资产的影响\n\n购买力不断蒸发的过程中，不同财富、商品、资产的增值速度是不同的。农产品、工业制成品等供给能力强则缺少增值能力；中国股市同样缺少增值能力，其重融资轻投资的特点总体是负和游戏，上证指数仍然在3000点左右，很难获得高收益升值保值能力强的如大城市学区房、茅台年份酒（供给有限）\n\n## 5.4 货币扩张对不同资产的影响\n\n美联储扩张资产负债表剥夺全球储蓄者，虽然补贴了美底层人民，但这救济金会迅速花掉，其实还是资本所有者获益。因为后危机时代，美国人工工资没有涨，除个别城市房价也没有涨，持续大涨的是股市（货币政策的强大再分配效应）。\n\n遇到的危机都可以通过美元霸权的铸币税效应，把代价转移给世界分摊，数万亿新增的美元欧元流动性流向全球外围地区商品和资产，  **引发全球普遍通胀和资产价格上涨。**\n\n 只有成为国际货币才能保护自己劳动创造的财富。用低息的国债置换大部分高息的地方政府债务，做大国债池子使其成为与美债相竞争的无风险资产池\n\n# 六、分析\n\n## 6.1 人民币汇率预判及分析方法\n\n年度出生人口如果明显下跌，汇率走强，短期关注美元指数的价格波动\n\n## 6.2 美元走强的情况\n\n美元率先欧洲、东亚经济复苏，美元会走强。全球经济低迷，美元会走强。如果美和世界同步进入比较好的经济发展状况时，美元指数会下跌。当世界经济出现危机，绝大部分流动性会躲到美国短期国债中去，美元指数反而会大幅上涨。次贷危机如此，疫情期间同样如此。\n\n## 6.3 半球模型与全球资产配置的大周期理论\n\n在美元下行周期，借入低息资本到外围新兴工业化区域投资（看哪个国家的国际收支平衡表上正在堆积越来越多的美元储备，买入最贵的房产和土地，一般十年为周期），当上涨周期到来，把当地房产卖掉后转到美国短期国债 。16年左右一个周期\n\n1970年，拉美  当时世界市场体系中心的大量投资者投资到拉美，带来当地资产价格的上升，同样1980年代初加息后，卖出当地房产、货币回归美短期国债。80年代中期，再次进入下行周期后，当趋势明确右肩交易，再去找哪些国家国际收支平衡表出现大量美元盈余，日本、韩国、台湾、新加披，进行相同的操作，到1996前后美元指数趋势性上涨，相同操作。2001年开始进行下行阶段，03年基本可以确定新一轮美元下行周期又开始了，跟着资金的流向，到了中国，到了2015年美元指数又开始上涨。\n\n","tags":["Exchange Rate"],"categories":["Economic"]},{"title":"Multimodal_learning","url":"/2023/04/12/Multimodal-learning/","tags":["Multimodal"]},{"title":"How to design experiments","url":"/2022/07/12/How to design experiments/","content":"\n# 如何设计深度学习实验\n\n## For insights, not for numbers!\n\n- 保证实验的泛化性\n  - 尽量在大数据集上验证\n  - 尽可能在多数据集上做（寻找问题）\n- 验证复现重要的结论\n  - 反复研究重要的工作（历史留名的工作）\n- 追求的多样化\n  - 方法能否更简单、跳出已有空间、有 **更好的视觉效果** （可视化结果）？\n- 大胆的猜想\n  - 根据已有实验结果做猜想，如`Attention is all you need`\n  - 能否做一些极端例子的检测（发现问题）\n    - 如何挖掘实验结果中隐藏的信息，需要平时看论文时深入思考分析，数据背后反映了一个什么问题，能找到的话就是一个很好的`idea`\n\n## 控制变量\n\n- 只验证一个变量\n  - 一次实验只改变一个参数\n- 发掘已有工作的问题\n  - 有没有没发掘出来的，有价值的研究点（如都用的trick却没人研究过）\n- 排除干扰因素\n  - 实验所有变量做好记录\n  - **版本控制要做好，一个commit只改动一个点,实验的不同版本提交到github并命名**\n- 总结变量的重要性\n  - 数据分布（idea如数据通路的处理、凸优化、贝叶斯等优化技巧）\n  - 前、后处理\n\n## 搭环境（与时俱进docker）\n\n- 系统层面\n  - Linux优先，windows考虑WSL\n  - 管理CPU/GPU资源：`slumr`调度系统\n  - terminal：`terminius`\n- 网络层面\n  - 熟悉SSH/SCP协议\n  - 熟悉远程编译\n  - 远程可视化：`jupyterlab`\n- IDE层面\n  - 常见功能：自动同步、`Github`版本管理、代码对比、自动保存历史\n\n## 验证IDEA\n\n- 确保任务的可行性（确保`baseline`可以跑出有意义结果）\n- 不断调整idea（方向一致如`deform cnn`）\n  - 什么是最有价值的部分\n  - 更好的可视化效果（找问题）\n- 分模块验证\n\n## 管理实验\n\n- 表格驱动的实验方法\n  - 实验前先做表格\n  - 记录主要实验变量，超参，`trick`\n  - 一组实验总结出一个结论，驱动下一组实验\n- 合理利用可视化工具\n- 关注早期的实验结果\n  - 可以固定一个小epoch进行比较，最后再跑完整个实验\n  - 结合可视化手段，分析每个阶段大概做的如何\n\n","tags":["paper"]},{"title":"Computer vision 1-4","url":"/2022/07/12/computer-vision/","content":"\n# 1. 人工智能的提出和发展历史\n\n## 1.1 正式诞生：达特茅斯会议（1956）\n\n## <img src=\"computer-vision/image-20220831133047984.png\" alt=\"image-20220831133047984\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831133252198.png\" alt=\"image-20220831133252198\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831133739312.png\" alt=\"image-20220831133739312\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831134112693.png\" alt=\"image-20220831134112693\" style=\"zoom:50%;\" />\n\n![1](computer-vision/1.png)\n\n可能并不需要真正定义一个符号系统（代表逻辑推理），引出后来的统计学习方法来做AI的思路（数据：题海战术），1993年后统计学习的本质并未改变。\n\n<img src=\"computer-vision/image-20220831135100304.png\" alt=\"image-20220831135100304\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831135317658.png\" alt=\"image-20220831135317658\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831135618400.png\" alt=\"image-20220831135618400\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831140621388.png\" alt=\"image-20220831140621388\" style=\"zoom:50%;\" />\n\n**如何利用常识？**\n\n## 1.2 人工智能的方法论\n\n**完全基于概率统计的模型**\n\n<img src=\"computer-vision/image-20220831151341825.png\" alt=\"image-20220831151341825\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831152503085.png\" alt=\"image-20220831152503085\" style=\"zoom:45%;\" />\n\n手工设计基于人类知识，自主学习基于数据，两者的使用主要取决于处理的任务需要，没有优劣之分。\n\n<img src=\"computer-vision/image-20220831153720038.png\" alt=\"image-20220831153720038\" style=\"zoom:50%;\" />\n\n**在满足可行性、优越性及必要性的前提下，什么情况使用深度学习的方法：**\n\n- 没有现有知识可以直接建模 \n- 复杂度过高\n\n<img src=\"computer-vision/image-20220831154749551.png\" alt=\"image-20220831154749551\" style=\"zoom:50%;\" />\n\n# 2. 计算机视觉\n\n## 2.1 计算机视觉的定义\n\n计算机视觉就是处理输入信号或输出信号是图像的问题，计算机图形学更偏向输出信号为图像。\n\n![image-20220831164227462](computer-vision/image-20220831164227462.png)\n\n<img src=\"computer-vision/image-20220831165418148.png\" alt=\"image-20220831165418148\" style=\"zoom:50%;\" />\n\n## 2.2 计算机视觉的发展历史\n\n![image-20220831165745186](computer-vision/image-20220831165745186.png)\n\n<img src=\"computer-vision/image-20220831170129270.png\" alt=\"image-20220831170129270\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831170326743.png\" alt=\"image-20220831170326743\" style=\"zoom:50%;\" />\n\n![image-20220831171401465](computer-vision/image-20220831171401465.png)\n\n<img src=\"computer-vision/image-20220831171939117.png\" alt=\"image-20220831171939117\" style=\"zoom:50%;\" />\n\n![image-20220831172624736](computer-vision/image-20220831172624736.png)\n\n<img src=\"computer-vision/image-20220831183900453.png\" alt=\"image-20220831183900453\" style=\"zoom:50%;\" />\n\n**小结：** 计算机视觉是人工智能的一个分支。由于图像的存储和处理需要消耗更多资源，计算机视觉的发展略微滞后于人工智能，但也同样精彩。计算机视觉的发展历史，是手工特征和神经网络（连结主义）不断角力的过程，而时下流行的深度学习正是连结主义的最强武器。同时，计算机视觉的发展还是时代的产物，受到大算力和大数据的极大推动。\n\n## 2.3 计算机视觉的典型问题和应用\n\n![image-20220831191725817](computer-vision/image-20220831191725817.png)\n\n摄像机标定：摄像机的内外参数在空间中体现如何，拍摄的物体有多远、多大。\n\n![image-20220831192458079](computer-vision/image-20220831192458079.png)\n\n密集表示：对人的身体进行切片，每一片对应身体的特定部位\n\n![image-20220831195928462](computer-vision/image-20220831195928462.png)\n\n## 2.4 计算机视觉的现状和挑战\n\n![image-20220831202536257](computer-vision/image-20220831202536257.png)\n\n![image-20220831204300503](computer-vision/image-20220831204300503.png)\n\n**长尾分布：** 少数类别有大部分数据，而多数类别只有小部分数据。往往会对头部数据过拟合，从而在预测时忽略尾部的类别。长尾效应无法通过指标体现，任何测试集都有自身的局限性。缺乏常识知识\n\n![image-20220831205329626](computer-vision/image-20220831205329626.png)\n\n<img src=\"computer-vision/image-20220831205958303.png\" alt=\"image-20220831205958303\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220831210241335.png\" alt=\"image-20220831210241335\" style=\"zoom:50%;\" />\n\n**某种指标在任务层面的过拟合难以避免**\n\n<img src=\"computer-vision/image-20220831210754916.png\" alt=\"image-20220831210754916\" style=\"zoom:50%;\" />\n\n![image-20220831211008729](computer-vision/image-20220831211008729.png)\n\n![image-20220831211220550](computer-vision/image-20220831211220550.png)\n\n## 2.5 计算机视觉与人工智能其他领域的联系\n\n![image-20220831211816045](computer-vision/image-20220831211816045.png)\n\n本节列举了：与计算机视觉互利的机器学习理论，与其互反的计算机图形学，以及与其互补的自然语言处理。未来可以从图像信号采样、标注、学习等方面推进。\n\n# 3. 统计学习基础\n\n## 3.1 统计学习的基本概念\n\n这一小节，主要讲述统计学习的基础知识。作者通过三个典型案例，即一维拟合问题、图像分类问题、游戏策略问题，讲述统计学习的通用流程和基本概念。**统计学习方法本质上是求解概率模型**，因而可以使用贝叶斯定理进行推导，并且**从概率角度定义生成式模型和判别式模型**。\n\n<img src=\"computer-vision/image-20220901095624670.png\" alt=\"image-20220901095624670\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220901100326862.png\" alt=\"image-20220901100326862\" style=\"zoom:50%;\" />\n\n![image-20220901100815657](computer-vision/image-20220901100815657.png)\n\n![image-20220901101503538](computer-vision/image-20220901101503538.png)\n\n**Note：**为什么不定义**一个**参数来而使用**十个**参数来做这件事？\n\n**补充说明：**\n\n- 数据分布（域）：决定了问题的难度甚至可行性，数据分布会影响评价指标、模型设计和优化。\n\n- 评价指标：决定了算法的好坏，在不均匀甚至长尾分布的数据上，不同的评价指标会导致截然不同的结果。长尾分布的数据虽少，但往往是决定算法是否可靠的关键。**没有完美的评价指标，指标随着任务的需求而变化**。\n\n- 模型复杂度和参数：解决同一问题的模型，可以有不同的复杂度（最近邻方法、线性回归法、CNN）。超参数是一种强先验，体现着人类对特定问题的理解。\n- 优化过程中的挑战：实际问题中，数据和参数的维度过高（不同维度重要性差别很大）；过拟合现象（把握合适的度非常困难）\n\n<img src=\"computer-vision/image-20220901111958019.png\" alt=\"image-20220901111958019\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220901112510039.png\" alt=\"image-20220901112510039\" style=\"zoom:50%;\" />\n\n概率模型：统计学习一般基于概率论和统计， **从数据分布的假设出发** ，利用贝叶斯定理，推出优化目标。\n\n<img src=\"computer-vision/image-20220901150044858.png\" alt=\"image-20220901150044858\" style=\"zoom:50%;\" />\n\n**其中求导过程两边取对数，参数只有一个其他均视为常数。**\n\n<img src=\"computer-vision/image-20220901151430449.png\" alt=\"image-20220901151430449\" style=\"zoom:50%;\" />\n\n**L1范数与损失函数的等高面相切的部分更有可能在坐标轴上，因此L1范数会使参数更稀疏？**\n\n<img src=\"computer-vision/image-20220901152038277.png\" alt=\"image-20220901152038277\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220901152923625.png\" alt=\"image-20220901152923625\" style=\"zoom:50%;\" />\n\n## 3.2 最近邻法和线性回归\n\n 在这一小节，作者主要讲述统计学习中两个最基础的方法，即最近邻法和线性回归法。最近邻法虽然简单直观，但由于缺乏参数的辅助，在高维空间中不易推广。引入参数后，最简单的模型是线性回归，它不仅具有解析解，而且能够自由地控制模型复杂度。这两种模型各有利弊，共同构成了统计学习的奠基部分。\n\n<img src=\"computer-vision/image-20220901204713297.png\" alt=\"image-20220901204713297\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220901205459359.png\" alt=\"image-20220901205459359\" style=\"zoom:50%;\" />\n\n最后一句不是很理解，如何推导\n\n<img src=\"computer-vision/image-20220901211226620.png\" alt=\"image-20220901211226620\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220901214308267.png\" alt=\"image-20220901214308267\" style=\"zoom:50%;\" />\n\n直观理解维度诅咒的例子：d维空间中将一个单位球半径为1减为0.99，那么减少了多少体积？体积与半径d次方成正比，即d很大，体积几乎为0。高维空间中的几乎所有的点，如果在体积中均匀采样的话，几乎所有点都位于球面的附近。\n\n<img src=\"computer-vision/image-20220901220812955.png\" alt=\"image-20220901220812955\" style=\"zoom:50%;\" />\n\n**最近邻法无法判断不同特征的重要程度，每个位置的权重都是一样的，无法学习特征的重要性。将数据投影到更高效的空间中，通过学习调节参数。**\n\n<img src=\"computer-vision/image-20220901222628377.png\" alt=\"image-20220901222628377\" style=\"zoom:50%;\" />\n\n<img src=\"computer-vision/image-20220902101227171.png\" alt=\"image-20220902101227171\" style=\"zoom:50%;\" />\n\n此处的线性反映在求导后**可变参数**都是一次的\n\n<img src=\"computer-vision/image-20220902110511451.png\" alt=\"image-20220902110511451\" style=\"zoom:50%;\" />\n\n多项式的次数L是一个超参数，在事先不知道目标函数时，多采用增大特征集的方式来提高拟合效果，过大会过拟合。带参模型的线性回归方法优于无参的最近邻法。\n\n<img src=\"computer-vision/image-20220902113500770.png\" alt=\"image-20220902113500770\" style=\"zoom:50%;\" />\n\n**为什么不使用一维的值作为函数值？** 预测的值在语义空间上没有连续性，无法充当分类结果。希望的y应该两两类别之间的距离一样？为什么需要让两两类别之间的距离一样？存疑\n\n是否使用`one-hot`编码取决于离散特征的取值之间有没有大小的意义，分类任务不同类别的值没有大小的意义。对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了`m`个二元特征，并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。使用`one-hot`编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用`one-hot`编码，**会让特征之间的距离计算更加合理**。\n\n<img src=\"computer-vision/image-20220902154338475.png\" alt=\"image-20220902154338475\" style=\"zoom:50%;\" />\n\n无参的最近邻法效果比线性回归法更好，分析现象产生的原因。\n\n最近邻法中每一个点控制了其附近的一块区域，而线性回归法在整个空间中划分。哪种方法合理呢？不是说有参数就好，在这种情况下，最近邻法虽然没有参数却可以更好的适应空间的性质。实验结果表明高维空间上让每个点控制一块区域比在整个空间上划线效果更好，推出参数不一定总能带来精度上的增益，更本质的是空间的复杂度不允许我们使用线性的分界面来做，因此模型虽然有参数了，但模型的设计还要符合实际的情况。\n\n**此处可以仔细思考作者的思考过程是如何针对两种方法效果的差异，推导出结论的。**\n\n<img src=\"computer-vision/image-20220902160836184.png\" alt=\"image-20220902160836184\" style=\"zoom:50%;\" />\n\n## 3.3 降维方法和统计学习三要素\n\n在这一小节，作者主要讲述降维方法，并据此引出统计学习的三个基本要素。原始的最近邻法和线性回归法都很难扩展到高维数据中，其主要原因是维度诅咒的存在。统计学习方法可以通过降维缓解这一问题，而降维的 **本质是将数据分布等信息存储于模型参数中，从而达到增加信息熵的目的。** 按照这种思路，统计学习的三要素可以确定为数据、模型、算力，而其他重要的元素与三要素之间有着密切的联系。\n\n<img src=\"computer-vision/image-20220902165450415.png\" alt=\"image-20220902165450415\"  />\n\n![image-20220902165553862](computer-vision/image-20220902165553862.png)\n\n![image-20220902165656676](computer-vision/image-20220902165656676.png)\n\n有没有方法可以在考虑语义监督的同时引入非线性呢？\n\n![image-20220902171240891](computer-vision/image-20220902171240891.png)\n\n两层神经网络可以视为主成分分析的改进\n\n其他著名降维方法：自编码机。利用降维后恢复原数据的方式，无监督地学习数据内在分布。变分自编码机，使得压缩后的数据满足某种特定分布，从而允许通过采样来生成高维数据。\n\n![image-20220902192818009](computer-vision/image-20220902192818009.png)\n\n相对关系的具体阐述见[A]，降维后数据之间的相对关系保留的很好。t-SNE是通过各种近似的方法优化，所以复杂度高；而PCA是个迭代过程使用矩阵运算即可。\n\n![image-20220902193744960](computer-vision/image-20220902193744960.png)\n\n![image-20220902200402833](computer-vision/image-20220902200402833.png)\n\n![image-20220902201818623](computer-vision/image-20220902201818623.png)\n\n## 3.4 模型优化的欠拟合和过拟合\n\n在这一小节，作者主要讲述模型优化的全景以及其中最常见的困难，即欠拟合和过拟合。模型优化是一个系统性工程，而当特征空间性质复杂、训练数据减少时，就很容易出现欠拟合和过拟合的问题。以一维拟合问题为例，可以观察到数据量和模型复杂度对于拟合程度的影响。**最后，将拟合程度归结为从偏差和方差的平衡，并且得到结论：没有最好的模型，只有最合适的模型。**\n\n![image-20220903094431285](computer-vision/image-20220903094431285.png)\n\n![image-20220903101525709](computer-vision/image-20220903101525709.png)\n\n![image-20220903103151604](computer-vision/image-20220903103151604.png)\n\n**多项式系数的大小可以作为模型是否过拟合的指标 ，正则化实际是约束了多项式系数。**\n\n![image-20220903104834449](computer-vision/image-20220903104834449.png)\n\n![image-20220903111042745](computer-vision/image-20220903111042745.png)\n\n![image-20220903111106516](computer-vision/image-20220903111106516.png)\n\n## 3.5两个经典的统计学习范式\n\n![image-20220903111612237](computer-vision/image-20220903111612237.png)\n\n![image-20220903112253774](computer-vision/image-20220903112253774.png)\n\n![image-20220903112512750](computer-vision/image-20220903112512750.png)\n\n![image-20220903112717413](computer-vision/image-20220903112717413.png)\n\n# 4. 视觉模型设计的基本原则\n\n在这一章，作者首先分析视觉信号的重要性质，并由此引出统计学习方法需要满足的三大基本原则。它们之间的对应关系是：视觉信号的语义对应关系复杂——层次性原则；视觉信号的语义信息密度低——最小描述原则；视觉信号连续变化且边界模糊——分散表示原则。\n\n## 4.1 层次性原则\n\n在这一小节，作者主要讲述第一个基本原则：层次性原则。图像信号的基本单元是像素，而像素和语义之间存在着巨大的语义鸿沟。通过简单的或者手工定义的函数，很难跨越语义鸿沟；往往需要使用分步设计的思想，构造基本操作的复合函数，来实现复杂映射关系。层次性原则催生了局部设计理念，也是深度学习的重要基石。\n\n![image-20220903144036642](computer-vision/image-20220903144036642.png)\n\n![image-20220903144900051](computer-vision/image-20220903144900051.png)\n\n性质2：图像的语义信息密度低\n\n![image-20220903150040371](computer-vision/image-20220903150040371.png)\n\n![image-20220903150903379](computer-vision/image-20220903150903379.png)\n\n![image-20220903151954608](computer-vision/image-20220903151954608.png)\n\n![image-20220903152347061](computer-vision/image-20220903152347061.png)\n\n了解词袋模型的局限性，对理解当前的深度学习方法是很有帮助的\n\n![image-20220903152323716](computer-vision/image-20220903152323716.png)\n\n![image-20220903153440107](computer-vision/image-20220903153440107.png)\n\n![image-20220903154653293](computer-vision/image-20220903154653293.png)\n\n![image-20220903155442632](computer-vision/image-20220903155442632.png)\n\n![image-20220903160129630](computer-vision/image-20220903160129630.png)\n\n这里作者再次强调了深度学习和强化学习今后一定能好的联系在一起，处理更复杂的问题。\n\n## 4.2最小描述原则\n\n在这一小节，作者主要讲述第二个基本原则：最小描述原则。图像信号的语义密度低，因此从统计学习角度看，从中抽取紧凑特征并且基于低维特征学习语义，能够降低模型的结构风险。最小描述原则在一系列统计学习模型中都有所体现。**从一般意义上说，它所追求的目标，是压缩率和恢复率间的权衡，但是如何衡量恢复效果，是一个触及视觉本质的难题。**\n\n![image-20220903195922075](computer-vision/image-20220903195922075.png)\n\n![image-20220903200929868](computer-vision/image-20220903200929868.png)\n\n![image-20220903200951677](computer-vision/image-20220903200951677.png)\n\n![image-20220903201606691](computer-vision/image-20220903201606691.png)\n\n压缩不够会过拟合，压缩过了又会丢失重要维度信息。\n\n![image-20220903201931005](computer-vision/image-20220903201931005.png)\n\n![image-20220903202316881](computer-vision/image-20220903202316881.png)\n\n![image-20220903203053503](computer-vision/image-20220903203053503.png)\n\n**最小描述原则的本质是数据的不均匀分布，只有不均匀的分布才能被压缩。**\n\n![image-20220903204112715](computer-vision/image-20220903204112715.png)\n\n![image-20220903204655091](computer-vision/image-20220903204655091.png)\n\n## 4.3 分散表示原则\n\n在这一小节，作者主要讲述第三个基本原则：分散表示原则。图像信号的域特性很强，因而不同场景的数据间分布差异巨大。分布间的变换通常不完全可控，而域迁移（迁移学习）就是处理不可控变换的有效手段。在深度学习的基础上，域迁移可以通过微调神经网络的参数来实现，而微调的力度则需要根据实际情况来确定。\n\n![image-20220904153014943](computer-vision/image-20220904153014943.png)\n\n![image-20220904154701746](computer-vision/image-20220904154701746.png)\n\n微调的参数越多，越容易过拟合。\n\n如何决定模型微调的力度？取决于域间差异、新域数据的多少。\n\n![image-20220904161232797](computer-vision/image-20220904161232797.png)\n\n**图像信号采样与语义的相关性很低，本质上还是缺乏常识**\n\n## 4.4 三大原则的结合\n\n![image-20220904163356690](computer-vision/image-20220904163356690.png)\n\n图像的基本性质决定了我们处理问题的原则，基本原则又催生了具体的解决方案，方案不一定是最好的，但指导原则是相对固定的。\n\n![image-20220904163807590](computer-vision/image-20220904163807590.png)\n\n区别二中指出了NLP长期领先CV的部分原因\n\n![image-20220904163940758](computer-vision/image-20220904163940758.png)\n\n**问题还在特征的表达上，我们并不知道特征和语义之间的关系，如何使特征和常识、知识联系起来。**\n\n![image-20220904164338781](computer-vision/image-20220904164338781.png)\n\n","tags":["course"]},{"title":"How to read","url":"/2022/07/01/Read/","content":"\n> An introduction to research paper reading\n\n# 批判阅读\n\n- 如果作者是尝试解决某个问题\n  - 那他们要解决的这个问题是正确的么？\n  - 有没有更简单的解法？\n  - 答案有什么局限性？\n- 如果作者呈现数据\n  - 他们是否有正确的数据来支撑论点？\n  - 他们是否以正确的方式来组织这些数据？\n  - 他们是否以合理的方式来解释了这些数据？\n  - 是否还有其他更具说服力的数据集？\n- 作者的假设合理吗？\n  - 逻辑清晰且公正吗？\n  - 理由有没有漏洞？\n\n\n# 创造性阅读\n\n- 文章还有哪些点是作者没想到的\n- 我是否可以对那些地方进行改进，从而提升效果\n- 能不能把这篇论文和其他的论文关联起来，从而产生新的想法，可以支撑下一阶段的研究\n\n如果真正想理解论文，可以写一个摘要，最好做一个报告。当把东西写下来或者说出来，才能说明我们真正理解了。\n\n# 问题清单\n\n- 论文的核心观点是什么？\n- 主要的局限性是什么？\n- 代码和数据是不是可用？\n- 这是一个好的想法吗？是否违反直觉？\n- 论文的贡献是否有意义？\n- 论文中的实验是否足够好？还可以调优吗？\n- 有没有错过什么相关论文么？\n- 对我的工作和产出有何帮助么？\n- 这是一篇值得关注的论文么？\n- 其他的人对这篇论文有何看法呢？\n- 这个研究领域的领头人是谁呢？\n- 如果有机会见到作者，我应该问作者什么问题？\n\n> 这里我引用两个人说的话，一个是杨振宁先生，他说他曾经看到过几千个研究者，有的10年后非常成功，有的却失败了，这并不是因为成功者更聪明，而是因为成功者找到了正确的方向，知道该做什么；还有就是上周看到了祥雨朋友圈分享了一个观点，我觉得还蛮有意思的：真正做一些伟大的东西，往往都需要很好的直觉——不是数学，不是理论推导，而是你根据历史脉络和广度得到的一些思考。带着这样的思考和信念去做科研其实是更简单的：（做科研）其实是一个Easy模式，而不是Hard模式。 --huhan\n\n> 或许你永远不知道你以前读过的书能在什么时候能够派上用场，但请保持阅读，因为阅读的过程也是在你大脑中建立认知的过程。\n>\n> 深度阅读论文，要敢于对论文质疑，质疑论文作者的研究方法、思路、技巧。还要设身处地去想：如果我来写这篇论文，我能用什么方法。 --Harry\n\n[沈向洋博士在全球创新学院（GIX）进行的直播“You are how you read”视频回放](https://www.bilibili.com/video/BV1df4y1m74k?spm_id_from=333.880.my_history.page.click&vd_source=1d323c0fb399b7e3d65130870e2e897b)\n","tags":["paper"]},{"title":"EdgeNext","url":"/2022/06/23/EdgeNext/","content":"\n>  目前最新的轻量级网络，整体结构清晰，代码可复现性强，其中的设计思路值得学习。在通道层面做自注意力将复杂度降低到线性的，同时利用深度可分离卷积构建空间上的上下文关系，分开建模两部分的信息。实验效果提升显著，保持高精度的同时大大降低了延迟和运算。\n\n#Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications\n\n## Overall Architecture\n\n![image-20220623154946031](EdgeNext/image-20220623154946031.png)\n\n## Conv Encoder\n\n```python\nclass ConvEncoder(nn.Module):\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=4, kernel_size=7):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(expan_ratio * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n        return x\n```\n\n## SDTA Encoder\n\n```python\nclass SDTAEncoder(nn.Module):\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=4,\n                 use_pos_emb=True, num_heads=8, qkv_bias=True, attn_drop=0., drop=0., scales=1):\n        super().__init__()\n        # width：每个分支的对应通道数\tscales：几个分支\n        width = max(int(math.ceil(dim / scales)), int(math.floor(dim // scales)))\n        self.width = width\n        if scales == 1:\n            self.nums = 1\n        else:\n            self.nums = scales - 1\t#需要的卷积数为分支数-1\n        convs = []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, kernel_size=3, padding=1, groups=width))\n        self.convs = nn.ModuleList(convs)\n\n        self.pos_embd = None\n        if use_pos_emb:\n            self.pos_embd = PositionalEncodingFourier(dim=dim)\n        self.norm_xca = LayerNorm(dim, eps=1e-6)\n        self.gamma_xca = nn.Parameter(layer_scale_init_value * torch.ones(dim),\n                                      requires_grad=True) if layer_scale_init_value > 0 else None\n        self.xca = XCA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)  # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()  # TODO: MobileViT is using 'swish'\n        self.pwconv2 = nn.Linear(expan_ratio * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n\n        spx = torch.split(x, self.width, 1)\n        # 以Stange2为例,self.nums=1, i只取0\n        for i in range(self.nums):\t\n            if i == 0:\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            if i == 0:\n                out = sp\n            else:\n                out = torch.cat((out, sp), 1)\n        x = torch.cat((out, spx[self.nums]), 1)\n        # XCA\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H * W).permute(0, 2, 1)\n        if self.pos_embd:\n            pos_encoding = self.pos_embd(B, H, W).reshape(B, -1, x.shape[1]).permute(0, 2, 1)\n            x = x + pos_encoding\n        x = x + self.drop_path(self.gamma_xca * self.xca(self.norm_xca(x)))\n        x = x.reshape(B, H, W, C)\n\n        # Inverted Bottleneck\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n\n        return x\n\n#通道方向的自注意力\nclass XCA(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\t\t\n        # (H*W, C) -> (C, H*W)\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        # -------------------\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n        # ------------------\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'temperature'}\n```\n\n## EdgeNeXt\n\n```python\nclass EdgeNeXt(nn.Module):\n    def __init__(self, in_chans=3, num_classes=1000,\n                 depths=[3, 3, 9, 3], dims=[24, 48, 88, 168],\n                 global_block=[0, 0, 0, 3], global_block_type=['None', 'None', 'None', 'SDTA'],\n                 drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1., expan_ratio=4,\n                 kernel_sizes=[7, 7, 7, 7], heads=[8, 8, 8, 8], use_pos_embd_xca=[False, False, False, False],\n                 use_pos_embd_global=False, d2_scales=[2, 3, 4, 5], **kwargs):\n        super().__init__()\n        for g in global_block_type:\n            assert g in ['None', 'SDTA']\n        if use_pos_embd_global:\n            self.pos_embd = PositionalEncodingFourier(dim=dims[0])\n        else:\n            self.pos_embd = None\n        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n        for i in range(4):\n            stage_blocks = []\n            for j in range(depths[i]):\n                if j > depths[i] - global_block[i] - 1:\n                    if global_block_type[i] == 'SDTA':\n                        stage_blocks.append(SDTAEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                        expan_ratio=expan_ratio, scales=d2_scales[i],\n                                                        use_pos_emb=use_pos_embd_xca[i], num_heads=heads[i]))\n                    else:\n                        raise NotImplementedError\n                else:\n                    stage_blocks.append(ConvEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                    layer_scale_init_value=layer_scale_init_value,\n                                                    expan_ratio=expan_ratio, kernel_size=kernel_sizes[i]))\n\n            self.stages.append(nn.Sequential(*stage_blocks))\n            cur += depths[i]\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # Final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head_dropout = nn.Dropout(kwargs[\"classifier_dropout\"])\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):  # TODO: MobileViT is using 'kaiming_normal' for initializing conv layers\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (LayerNorm, nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_features(self, x):\n        x = self.downsample_layers[0](x)\n        x = self.stages[0](x)\n        if self.pos_embd:\n            B, C, H, W = x.shape\n            x = x + self.pos_embd(B, H, W)\n        for i in range(1, 4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n\n        return self.norm(x.mean([-2, -1]))  # Global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(self.head_dropout(x))\n        return x\n```\n\n## Experiments\n\n![image-20220623164213601](EdgeNext/image-20220623164213601.png)\n\n![image-20220623164312032](EdgeNext/image-20220623164312032.png)\n\n> [论文地址](https://readpaper.com/pdf-annotate/note?pdfId=699460569366302720&noteId=699501058328915968)\n>\n> [source code](https://github.com/mmaaz60/EdgeNeXt)\n\n","tags":["paper"],"categories":["paper"]},{"title":"mmcv","url":"/2022/06/19/mmcv/","content":"\n# mmcv功能概览\n\n![image-20220619194629220](mmcv/image-20220619194629220.png)\n\n- ​\t配置文件的可读性比原始的好，直接反映了配置和实现的关系。上图左中`type`表示此模型为`RetinaNet`，后续字典都是其属性。\n\n- ​    配置文件通过继承更清晰其中变动之处，不用每次去找不同之处了。\n\n- ​\t不仅支持`python`格式的配置文件，也支持`yaml`等格式，但灵活性没那么好。\n\n  ![image-20220619195356822](mmcv/image-20220619195356822.png)\n\n配置文件中的各种参数代表了`RetinaHead`在构造时对应的值。其中绿色虚线框对应`**kwargs`，这些属性并不是给`RetinaHead`使用的，而是给其父类的。我们在`RetinaHead`中会调用父类的方法，利用`**kwargs`将父类对应的属性初始化\n\n![image-20220619200037241](mmcv/image-20220619200037241.png)\n\n`RetinaHead`仅仅只是`RetinaNet`中的一个字段，即传入的最外层传入一个`type`内层都是带`type`结构的子结构字典。我们只要保证在具体类的实现中，内部有去解析这些字典。对应图中`RetinaNet` 父类`SingleStangeDetector`中通过`build_backbone`在内部完成了对应属性的实例化\n\n![image-20220619201554085](mmcv/image-20220619201554085.png)\n\n`_base_`申明一个配置文件列表，会将所有配置文件中定义的变量加载进来。其中`model`是对父类中内部字段**进行修改而不是覆盖**。\n\n只有在其中加入`_delete_`属性才会覆盖父类中对应字段\n\n配置文件通过一个列表来表示，但不能显示的知道配置文件中到底由哪些组件构成。vscode插件`Config View`可以解决这个问题\n\n## Config与Registry\n\n![image-20220619203517389](mmcv/image-20220619203517389.png)\n\n从`Config`中取对应字段的配置信息**还没有将其变成具体的实例只有解析功能**，要通过`Registry`将对应实例构建出来。\n\n![image-20220619204730643](mmcv/image-20220619204730643.png)\n\n根据`type`字段将`cfg`粗略分为两个类型。`type`类型：**训练时该变量的实例化后类型，其余字段为该变量实例化所需的参数。**`Registry `会根据`type` 和其他参数将变量实例化。\n\n`mmcv`训练最外层的抽象是`runner`，其有非常多的属性，如果在配置文件中写入runner，会导致嵌入很多层级的字典，阅读不便。所以在可读性和一一映射关系上做了取舍。对于`dataloader`和`dataset`为了防止文件嵌套层级过深，只将一些显而易见的属性写在最外层，不将这些暴露给用户 。\n\n![image-20220619210213182](mmcv/image-20220619210213182.png)\n\n![image-20220619210605888](mmcv/image-20220619210605888.png)\n\n- 左侧的方式每次构建完模型后都要手动向`model_factory`中加入一个字段。\n\n- 右侧是个`Registry`的最简形式。每个`Registry`实例都会有一个`module_dict`字典，利用一个装饰器实现将字符串与模型类的映射会自动添加到`module_dict`中，多一个模型只需多一行注册，便于维护。\n\n- 注册功能的具体用法：一种是装饰器的写法，只要对应脚本被执行（本身在运行该脚本或者在运行其他脚本时`import`该脚本，触发任意一种就完成注册行为）\n\n  一种是注册外部模块的写法，将其作为函数使用。\n\n![image-20220619211754014](mmcv/image-20220619211754014.png)\n\n`register_module`方法中可以传参，意味着不仅可以注册自己写的模块还可以注册外部的模块。\n\n![image-20220619212028484](mmcv/image-20220619212028484.png)\n\n正因为`registry`可以注册外部模块我们才能在配置文件中调用到这些模块。\n\n以上注册代码都写在某个模块中 ，要注册这些模块必须要运行对应脚本。\n\n![image-20220620201643776](mmcv/image-20220620201643776.png)\n\n![image-20220620201933637](mmcv/image-20220620201933637.png)\n\n测试代码的注册过程在脚本执行过程就完成了注册，训练中注册过程可通过`import`。\n\n侵入式注册：这种链式的行为导致`mmdet`更新后可能会与本地分支产生冲突。非侵入式注册则的方式则不需要修改对应的算法库。\n\n![image-20220620203346466](mmcv/image-20220620203346466.png)\n\n![image-20220620203652989](mmcv/image-20220620203652989.png)\n\n![image-20220620204435092](mmcv/image-20220620204435092.png)\n\n> [Config&Registry背后的故事ppt](https://openmmlab.feishu.cn/file/boxcnxBOiTaNOjPojDihE9vBDRd)\n>\n> [视频讲解](https://www.bilibili.com/video/BV19Z4y1q7Q5?spm_id_from=444.41.list.card_archive.click&vd_source=1d323c0fb399b7e3d65130870e2e897b)\n","tags":["code"],"categories":["source code"]},{"title":"DETR","url":"/2022/06/10/DETR/","content":"\n# End-to-End Object Detection with Transformers\n\n> 第一个端到端的检测器，没有了anchor和nms后处理，简化了流程。不过训练时间长，小物体检测效果差。DETR的成功主要还是Transformer的成功，之前也试过基于集合的目标函数、Encoder-Decoder的架构效果都不好，主要原因是特征不够好。\n\n![image-20220610100946952](DETR/image-20220610100946952.png)\n\n**主要贡献：** \t\n\n- **新的目标函数**（通过二分图匹配的方式输出一组预测，替代了原来的`nms`）\n\n- ` Encoder-Decoder Architecture`的架构 (预测是并行出框，不同于以往的自回归预测)\n\n  ## Introduce\n\n![image-20220610102211789](DETR/image-20220610102211789.png)\n\n**训练过程如下：**\n\n- `CNN`：抽取特征\n- `Transformer encoder`：每个特征会和图中其他所有特征交互，这样网络大概知道哪块是哪个物体，对同一个物体只出一个框，所以这种全局建模的方式有利于移除冗余的框。\n- `Decoder` ：生成预测框\n\n- 预测框和GT框做匹配：将这个过程看成集合预测的问题，在匹配上的框上作loss。\n\n推理过程中置信度大于0.7的物体才会被保留\n\n## Model\n\n###  Set prediction loss\n\n![image-20220610110729432](DETR/image-20220610110729432.png)\n\n![image-20220610111802283](DETR/image-20220610111802283.png)\n\n二分图匹配的例子：如何分配一些工人做一些工作使最后支出最小? 最优二分图匹配即最后有唯一解达成目标且成本最低。遍历算法来解决复杂度太高，常用匈牙利算法即`Scipy`包中的`linear-sum-assignment`函数来完成，该函数的输入就是`Cost matrix`, 输出即最优排列。可以将100个预测框视为`a、b、c`，GT框视为`x、y、z` ，检测中`Cost matrix`是损失值。最终输出的是与`GT`唯一匹配的预测框。\n\n![image-20220610112114389](DETR/image-20220610112114389.png)\n\n![image-20220610112446959](DETR/image-20220610112446959.png)\n\n![image-20220610112533981](DETR/image-20220610112533981.png)\n\n完成了这个最优匹配操作，就可以计算一个真正的目标函数loss，从而更新模型参数。\n\n### DETR architecture\n\n![image-20220610112641919](DETR/image-20220610112641919.png)\n\n### Visualizing\n\n![image-20220610122822730](DETR/image-20220610122822730.png)\n\n![image-20220610124936036](DETR/image-20220610124936036.png)\n\n`Encoder`越深不同物体的区分性越好\n\n![image-20220610122850085](DETR/image-20220610122850085.png)\n\n**可视化结果：** 仅使用`Transformer Encoder`图像中的物体已经有很好的区分了，再此基础上做`Decoder`后效果更佳。编解码一个都不能少，`Encoder`在学习一个全局的特征，将物体与物体区分开；`Decoder`在前面的基础上只需对头、尾巴等对象边界特征进行学习以解决遮挡问题来更好的区分物体\n\n![image-20220610131413686](DETR/image-20220610131413686.png![image-20220610132117871](DETR/image-20220610132117871.png)\n\n上图为`object query`的可视化，每个正方形代表一个`object query`，替代了anchor的生成机制，不同在于它是自己学的。以第一个为例，该`object query`学到最后，会问每个输入图片在左下角有没有看到小物体，在中间有没有看到横向的大物体，有的话告诉我。这100个`object query`相当于100个不停问问题的人，得到的答案就是目标框。\n\n```python\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet50\n\nclass DETR(nn.Module):\n    def __init__(self, num_classes, hidden_dim, nheads,8 num_encoder_layers, num_decoder_layers):\n        super().__init__()\n        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n        self.transformer = nn.Transformer(hidden_dim, nheads,14 num_encoder_layers, num_decoder_layers)\n        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n        self.linear_bbox = nn.Linear(hidden_dim, 4)\n        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        \n    def forward(self, inputs):\n        x = self.backbone(inputs)\n        h = self.conv(x)\n        H, W = h.shape[-2:]\n        pos = torch.cat([26 self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1)\n        h = self.transformer(pos + h.flatten(2).permute(2, 0, 1), self.query_pos.unsqueeze(1))\n        return self.linear_class(h), self.linear_bbox(h).sigmoid()\n    \ndetr = DETR(num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)\ndetr.eval()\ninputs = torch.randn(1, 3, 800, 1200)\nlogits, bboxes = detr(inputs)\n        \n\n```\n\n","tags":["paper"],"categories":["paper"]},{"title":"Vision GNN","url":"/2022/06/06/VisionGNN/","content":"\n# Vision GNN: An Image is Worth Graph of Nodes\n\n> 将图结构应用于图像领域，将VIT中的patches视作nodes。该网络主要由`GCN`和`FFN`组成。其中`GCN`模块来聚合、处理图结构信息；`FFN`模块对节点特征作变化，获取节点信息的多样性。[论文链接](https://readpaper.com/pdf-annotate/note?noteId=693400681528512512&pdfId=4630291798619602945)\n\n![image-20220606230827512](VisionGNN/image-20220606230827512.png)\n\n   如上图所示，节点之间的连接由其内容确定，不受位置关系限制。网格、序列都可以看成是图的特例。\n\n![image-20220607131156703](VisionGNN/image-20220607131156703.png)\n\n上图是图片如何表示成图的一个例子。蓝色点 表示`adjacent matrix`，白色点表示无连接；通常是很大的`sparse matrix`。\n\n> 图片出处：[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n\n#  How to transform an image to a graph\n\n##   VIG\n\n![image-20220606231533521](VisionGNN/image-20220606231533521.png)\n\n   网络的关键在于：如何将图像表示为图结构、如何学习图结构中的视觉表征\n\n### Graph Representation of Image\n\n![image-20220606232134301](VisionGNN/image-20220606232134301.png)\n\n此处`K取9~15`效果最佳\n\n### GCN\n\n  图卷积通过聚合相邻节点的特征来实现节点之间的信息交换。\n\n![image-20220606233416153](VisionGNN/image-20220606233416153.png)\n\n   具体的说图卷积分为聚合和更新两个操作。\n\n- ` aggregation operation`: 操作通过聚合邻接节点的特征来计算当前节点的表示\n- ` multi-head update operation`: 将聚合后的特征分为`h`个头，每个头对应不同的更新权重，有利于特征的多样性表达。\n\n![image-20220606234233716](VisionGNN/image-20220606234233716.png)\n\n​    `GCN`在网络变深后会有**过平滑现象**，这会降低节点特征的区分度，导致视觉识别性能下降。本文在**图卷积后**的每个节点内引入了更多的特征变化，促增加节点特征多样性。 \n\n![image-20220606235817431](VisionGNN/image-20220606235817431.png)\n\n   本文对应的解决策略如公式6所示。**为了进一步提高特征转换能力和缓解过平滑现象，本文在每个节点上使用`FFN`**，过程如下所示。\n$$\nZ = σ (Y W1 ) W2 + Y\n$$\n   基于本节的`graph representation of images` 和`ViG block`，VIG网络可以随着网络的加深，保持特征的多样性，从而学习判别性的表示。\n\n##  Experiments\n\n![image-20220607124600794](VisionGNN/image-20220607124600794.png)\n\n![image-20220607124657067](VisionGNN/image-20220607124657067.png)\n\n![image-20220607124921992](VisionGNN/image-20220607124921992.png)\n\n### Visualization\n\n![image-20220607125000107](VisionGNN/image-20220607125000107.png)\n\n   **可视化结果：浅层邻接节点的选取基于低级、局部特征如：颜色和纹理，深层中邻接节点的选取更依赖于语义等高级特征。**\n\n### Pseudocode\n\n![image-20220607130309524](VisionGNN/image-20220607130309524.png)\n\n<img src=\"VisionGNN/image-20220607130529777.png\" alt=\"image-20220607130529777\" style=\"zoom:150%;\" />\n\n<img src=\"VisionGNN/image-20220607130639174.png\" alt=\"image-20220607130639174\" style=\"zoom:150%;\" />\n","tags":["paper"],"categories":["paper"]},{"title":"detection","url":"/2022/06/05/detection/","content":"\n# 一、数据格式\n\n## 1. MS COCO\n\n```markdown\n├── coco2017: 数据集根目录\n     ├── train2017: 所有训练图像文件夹(118287张)\n     ├── val2017: 所有验证图像文件夹(5000张)\n     └── annotations: 对应标注文件夹\n     \t\t  ├── instances_train2017.json: 对应目标检测、分割任务的训练集标注文件\n     \t\t  ├── instances_val2017.json: 对应目标检测、分割任务的验证集标注文件\n     \t\t  ├── captions_train2017.json: 对应图像描述的训练集标注文件\n     \t\t  ├── captions_val2017.json: 对应图像描述的验证集标注文件\n     \t\t  ├── person_keypoints_train2017.json: 对应人体关键点检测的训练集标注文件\n     \t\t  └── person_keypoints_val2017.json: 对应人体关键点检测的验证集标注文件夹\n```\n\n```python\n# 读取jason文件\nimport json\n\njson_path = \"/data/coco2017/annotations/instances_val2017.json\"\njson_labels = json.load(open(json_path, \"r\"))\nprint(json_labels[\"info\"])\n\n\"\"\"\njson文件主要包含以下五个字段:{\n\"info\": {dict: 6}, # dict 数据集描述\n\n\"licenses\": [list: 8], # 内部是dict\n\n\"images\": [list: 5000], # 列表中每个元素都是一个dict,主要包括file_name, height, width，id是图片的唯一标识\n\n\"annotations\": [list: 36781], # 列表元素个数对应数据集中所有标注的目标个数，列表中每个元素都是一个dict对应一个目标的标注信息。包括目标的分割信息（polygons多边形）、目标边界框信息[x,y,width,height]（左上角x,y坐标，以及宽高）、目标面积、对应图像id以及类别id等\n\n\"categories\":  {list：80} ，# 内部是dict ,  主要字段包括类别id、类别名称和所属超类。\n}\n\"\"\"\n\n# segmentation字段：polygon以及RLE\ndef convert_coco_poly_mask(segmentations, height, width):\n    masks = [] \n    for polygons in segmentations:\n        rles = coco_mask.frPyObjects(polygons, height, width)\n        mask = coco_mask.decode(rles) # 此处的mask为二值蒙版,背景为0,前景均为1\n        if len(mask.shape) < 3:\n            mask = mask[..., None]\n        mask = torch.as_tensor(mask, dtype= torch.uint8)\n        mask = mask.any(dim = 2) # 通道方向只要有一个元素为1,即为True\n        masks.append(mask)\n    if masks:\n        masks = torch.stack(masks, dim = 0)\n    else:\n        # 如果mask为空，则说明没有目标，直接返回数值为0的mask\n        masks = torch.zeros((0, height, width), dtype = torch.uint8)\n    return masks\n```\n\nNote: coco格式下分割标签对应的标注文件为json格式，而voc格式中分割标签对应的是.png的图像。注文件（.png）的每个目标处的像素值是按照xml文件中目标顺序排列的，如xml中有三个目标则对应目标的像素值为1，2，3。\n\n## 2. YOLO\n\n- `yolo/`\n  - `images`\n    - `.txt` \t\t# 类id、x_center、y_center、w、h（真实像素值除以图片的高和宽之后的值） \n  - `labels`\n    - `.jpg`\n\n```python\n# COCO 格式的数据集转化为 YOLO 格式的数据集\n# --json_path 输入的json文件路径\n# --save_path 保存的文件夹名字，默认为当前目录下的labels\n\nimport os\nimport json\nfrom tqdm import tqdm\nimport argparse\n\nparser = argparse.ArgumentParser()\n# 这里根据自己的json文件位置，换成自己的就行\nparser.add_argument('--json_path',\n                    default=r'your path', type=str,\n                    help=\"input: coco format(json)\")\n# 这里设置.txt文件保存位置\nparser.add_argument('--save_path', default=r'your path', type=str,\n                    help=\"specify where to save the output dir of labels\")\narg = parser.parse_args()\n\ndef convert(size, box):\n    dw = 1. / (size[0])\n    dh = 1. / (size[1])\n    x = box[0] + box[2] / 2.0\n    y = box[1] + box[3] / 2.0\n    w = box[2]\n    h = box[3]\n    # round函数确定(xmin, ymin, xmax, ymax)的小数位数\n    x = round(x * dw, 6)\n    w = round(w * dw, 6)\n    y = round(y * dh, 6)\n    h = round(h * dh, 6)\n    return (x, y, w, h)\n\n\nif __name__ == '__main__':\n    json_file = arg.json_path  # COCO Object Instance 类型的标注\n    ana_txt_save_path = arg.save_path  # 保存的路径\n\n    data = json.load(open(json_file, 'r'))\n    if not os.path.exists(ana_txt_save_path):\n        os.makedirs(ana_txt_save_path)\n\n    id_map = {}  # coco数据集的id不连续！重新映射一下再输出\n    with open(os.path.join(ana_txt_save_path, 'classes.txt'), 'w') as f:\n        # 写入classes.txt\n        for i, category in enumerate(data['categories']):\n            f.write(f\"{category['name']}\\n\")\n            id_map[category['id']] = i\n    # print(id_map)\n    # 这里需要根据自己的需要，更改写入图像相对路径的文件位置。\n    list_file = open(os.path.join(ana_txt_save_path, 'train2017.txt'), 'w')\n    for img in tqdm(data['images']):\n        filename = img[\"file_name\"]\n        img_width = img[\"width\"]\n        img_height = img[\"height\"]\n        img_id = img[\"id\"]\n        head, tail = os.path.splitext(filename)\n        ana_txt_name = head + \".txt\"  # 对应的txt名字，与jpg一致\n        f_txt = open(os.path.join(ana_txt_save_path, ana_txt_name), 'w')\n        for ann in data['annotations']:\n            if ann['image_id'] == img_id:\n                box = convert((img_width, img_height), ann[\"bbox\"])\n                f_txt.write(\"%s %s %s %s %s\\n\" % (id_map[ann[\"category_id\"]], box[0], box[1], box[2], box[3]))\n        f_txt.close()\n```\n\n## 二、iou及nms\n\n```python\nimport torch\n\ndef iou(box, boxes, box_format=\"corners\"):\n    \"\"\"\"\n    Calculates intersection over union\n\n    parameters:\n        box (tensor): (1, 4)\n        boxes (tensor) : (N, 4)\n        corners : (x1, y1, x2, y2)\n\n    Return:\n        tensor: Intersection over union for all examples\n    \"\"\"\n    box1_x1 = box[..., 0:1]\n    box1_y1 = box[..., 1:2]\n    box1_x2 = box[..., 2:3]\n    box1_y2 = box[..., 3:4]\n    box2_x1 = boxes[..., 0:1]\n    box2_y1 = boxes[..., 1:2]\n    box2_x2 = boxes[..., 2:3]\n    box2_y2 = boxes[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n\ndef nms(boxes, iou_threshold):\n    \"\"\"\"\n    Non Max Supression given boxes\n\n    Parameters:\n        boxes (list): [class, conf, x1, y1, x2, y2]\n        iou_threshold (float)\n\n    Return:\n        list: boxes after nsm\n    \"\"\"\n\n    boxes = sorted(boxes, key = lambda x: x[1], reverse=True)\n    boxes_after_nms = []\n    while boxes:\n        chosen_box = boxes[0]\n\n        boxes = [\n            box\n            for box in boxes\n            if iou(\n                chosen_box[2:],\n                box[2:],\n            )\n            < iou_threshold\n        ]\n\n        boxes_after_nms.append(chosen_box)\n    return boxes_after_nms\n```\n\n","tags":["code"]},{"title":"yiyu","url":"/2022/05/23/yiyu/","content":"\n## 聊赖\n\n读者，作者\n\n最是无为\n\n心绪\n\n止于欣赏\n\n\n\n对于你\n\n我为自己的浅薄而感到羞愧\n\n想象着你的风采\n\n你不会刻意寻找\n\n我亦如是\n\n\n\n咿\n\n原来不只我一人\n\n这样想\n\n我俩缘分\n\n点到为止\n\n\n\n遗憾\n\n怎会呢\n\n开怀至倾溢都来不及呢\n\n\n\n午后闷热\n\n没有风\n\n## 非罔\n\n己酉月\n\n夜沉凉正\n\n核酸过了\n\n\n\n上犹江对岸\n\n无声子衿\n\n廿几，什么也不是\n\n拟稿搁置无兴致\n\n比上不足惹人羞\n\n\n\n吉光，吉光呀\n\n看样子是就这样下去了\n\n再外出寻觅的契机渺茫了\n\n平日里乐子少有的\n\n除非外街上整点生煎\n\n\n\n有时，朝夕真不如一行俳句\n\n有事，俳句\n\n真不如二两生煎\n\n\n\n吉光啊，吉光\n\n怎么找你\n\n找不到\n\n","tags":["serendipity"],"categories":["literature"]},{"title":"Spirituality","url":"/2022/05/22/Spirituality/","content":"\n# 参半\n\n> 鲜有良朋，贶也永叹——《诗经》\n>\n> （“贶”，音同“况”，赐的意思），取《诗经》，意思是少有朋友和我长叹长谈了。\n\n> 微神之躬，胡为乎泥中——《诗经》\n>\n> 若不是为了你的缘故，我不会在泥中打滚\n\n> 凡是令我倾心的书，都分辨不清是我在理解它呢还是它在理解我。\n\n>我爱的物、事、人，是不太提的。我爱音乐，不太听的。我爱某人，不太去看他的。现实生活中遇到他，我一定远远避开他。这是我的乖僻，是为了更近人情。\n\n>哲学、科学，拆了宗教的台，哲学成了控告宗教的原告，科学在旁边做证人。艺术，做了无神论的最高榜样，不仅否认神，还取代了神，不仅取消了神的诺言，还自己创造诺言，立即在现世兑现。\n\n>要相信相对真实。夫妻的意思，就是凭道义、义务，共同生活，是守约，不能去要求爱情。爱情，是青春、美貌、神秘。夫妻呢，是有福同享、有难同当。\n>\n>真够悲凉的，鉴于先生终生一人，此句我只赞同一半。\n\n> 生离等于死别\t庸凡不知\t忽生离而恸死别。\n\n> 尚无知识、经验、能力时，要竭力自尊，抗御邪恶，真是艰辛。\n\n> 文明的发展一方面使一切美好有更坚实的发展基础，但是同时也生出另一种力量来腐蚀美好。这种腐蚀的力量在贫困社会中也存在，但在富裕社会中要强大得多，往往构成一种难以抑制的力量。\n\n> 幽默是来自人对一种事物的高超的把握，是对一种知识的随心所欲的驾驭，是对一种材料的透彻的认识。一个人的幽默感，最基本的来源是个人在日常生活中的态度。一个人在日常的生活中，能否对多边的世界，对自己不能左右的事务，对自己的成功和失败，对待一切，持一种幽默的态度，是一个人的幽默感生长的基础。\n\n> 在这个纷繁的世界上生存的基本依据是什么？从人本身来说，从现代的观念来说，基本的是人所有的人格。世界是如此，社会是如此，人生是如此，辩论也概莫能外。\n\n> L在几年前曾经问过我“：你每天读书有意义吗？”我说“：你知道和尚为什么要念经吗？”我在信中说：“难得有人还想起我的幸福的问题，心中十分感动。我们这类人，已经习惯了无幸福的生活，也就是生活的平淡。没有幸福的欲望，也就没有痛苦。痛苦往往是伴随着对幸福的追求的，而不是伴随着幸福本身。所以要怀疑的不是幸福本身，而是每个人主观状态中的希望。我的问题还是：‘和尚为什么要念经？’这个问题能够回答，一切就归于平淡。叫做和平养无限天机。幸福之事，可欲而不可求，可求而不可执。修练了这么多年，有足够的空间在心中，无所谓幸福。当然我不反对他人追求幸福，因为人不可能过同样的生活。”\n\n> 缘起性空。佛说世界，即非世界，是名世界。\n>\n> 各家经义，不过是为了给脆弱的人心一个栖息的空间罢了\n\n> 我想，如果一定要刨根究底的话，它可能发源于我对生活的一种体验。因为我发现，人们看待一个有道德的人，透出的目光是亲切；看待一个有权力的人，透出的目光是敬畏；只有看待一个有知识的人，目光中才盈溢着无限的尊重，并饱含着由衷的敬佩。我向往那份尊重和敬佩。再有，我很欣赏书页中那一片于喧闹、复杂和浮噪之中仍可守的宁静的小天地。老师们，敬祈你们体谅我，满足我也许是奢侈但不失合理的要求“：往上读。”\n>\n> 写得真好，读书在整个社会如果能够得到这样的认识，那社会就会发展进步。可惜，人们往往要经过千辛万苦之后才能明白这一点。\n\n> 生活在这个世界上的人：有的是弱者；有的是强者；有的要别人来设定目标，有的给别人设定目标；有的需要感情支持生活，有的需要意志支持生活。我大概在每一对概念中都会选择做后一种人。\t\t\t\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n**——待续**\n","tags":["serendipity"],"categories":["literature"]},{"title":"Image_compress","url":"/2022/05/19/imagecompress/","content":"\n# JPEG 不可思议的压缩率\n\n> 以JPEG图像压缩算法为例，讲述信号处理的核心思想和主题（还讲了颜色空间、`YCbCr`、色度子采样、离散余弦变换、量化和无损编码等细节。)\n\n## 引子\n\n![image-20220519152830270](imagecompress/image-20220519152830270.png)\n\n**核心问题：** 图片压缩过程中哪类信息可以去除，怎么去除\n\n![image-20220519153141089](imagecompress/image-20220519153141089.png)\n\n科学家发现人眼对亮度更敏感对颜色没那么敏感——JPEG压缩方案考虑了这个特点——想要知道怎么做必须深入探索色彩空间领域。\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/colorspace.png\"></div>\n        <div style=\"display:table-cell; width:27%;\"><img src=\"imagecompress/color.png\"></div>\n    </div>\n</div>\n\nRGB色彩空间有个特点：如果沿着对角线走从原点走到（255，255，255），就能得到逐渐变亮的颜色，这些点构成的线定义了所有可能的灰阶颜色，这是衡量亮度最直接的方式。将亮度独立出来的想法是另一个色彩空间的基石`YCbCr` ,`Y`分量衡量的是图片的“流明”或亮度，`Cb`和`Cr`存储的是颜色信息。`Y`可以视为一个单独的纵轴，值越大表明亮度越大。JPEG使用的这种色彩空间的原因是让我们能直接调用最适合人眼识别的颜色。\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/1.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:26%;\"><img src=\"imagecompress/2.png\"></div>\n    </div>\n</div>\n\n\n由于人对亮度敏感度高，其中一个压缩原图的思路就是缩减`Cb`和`Cr`分量的采样数，但把亮度分量全保留。这个技术被称为“色度下采样/色度抽样”。\n\n\n\n------\n\n\n\n## 色度下采样/色度抽样\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:25%\"><img src=\"imagecompress/3.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:55%;\"><img src=\"imagecompress/4.png\"></div>\n    </div>\n</div>\n\n`色度下采样`的具体操作是对原图种四个像素点求平均得到一个值；`色度抽样`通常选左上角像素值代表整个2×2区域的颜色，待色彩分量的采样数缩减后就可以和亮度分量合并如上右图所示。在这里色彩分量保持16个像素，这就得到了抽样后的图像。\n\n<img src=\"imagecompress/image-20220519161938690.png\" alt=\"image-20220519161938690\"  />\n\n至此距离JPEG 95%的压缩率仍有距离，需要考虑其他方面\n\n![image-20220519162214777](imagecompress/image-20220519162214777.png)\n\n## Y\n\n下面的压缩环节我们关注`Y`通道,本节的核心原理同样可以用于色彩分量\n\n### DCT概述\t\n\n我们需要以完全不同的视角来看待图片，有一个看待图片的角度是将图片视作“信号”\n\n![image-20220519162908961](imagecompress/image-20220519162908961.png)\n\n取出图片中若干像素构成的一行，使我们得以讨论图片里的“频率分量”\n\n![image-20220519163136283](imagecompress/image-20220519163136283.png)\n\n![image-20220519163255418](imagecompress/image-20220519163255418.png)\n\n人类视觉系统通常对图片的高频细节不敏感，因此JPEG会有策略地去除图片中不太重要、较少见的高频信号，达到更大的压缩率。**那么如何提取频率分量呢？**\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:33%\"><img src=\"imagecompress/5.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:30%;\"><img src=\"imagecompress/6.png\"></div>\n    </div>\n</div>\n\n\n先将这8个像素当成是某种信号，`DCT`将原始信号的**样本点**作为输入。\n\n![image-20220519165127117](imagecompress/image-20220519165127117.png)\n\n![image-20220519165454818](imagecompress/image-20220519165454818.png)\n\n用“系数”来描述`DCT`的输出,==**这些系数描述了组成原始信号的不同频率余弦波的权重**== 。本质是求出某个特定的余弦波应在信号里包含多少。\n\n可以把这个过程类比为将复杂的信号拆分成简单余弦波的加权求和，**那么到底使用了哪些余弦波，波如何和图中的像素建立关联？**以下取cos(x)为例\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:40%\"><img src=\"imagecompress/7.png\"></div>\n        <div style=\"display:table-cell; text-align:right; width:26%;\"><img src=\"imagecompress/8.png\"></div>\n    </div>\n</div>\n\n\n\n为了与之前的例子保持一致，我们需要在余弦波中采8个点做样本\n\n![image-20220519165454818](imagecompress/9.png)\n\n输出表明对输入有贡献的余弦波只有一个，看似合理因为此时的输入本身就取自一个余弦波。==改变余弦波的振幅==发现`DCT`系数`X1`同比变化，说明其实际担任了余弦曲线中权重的角色。**那么如何将其与图像联系起来？**\n\n![image-20220519172027023](imagecompress/image-20220519172027023.png)\n\n**构建余弦波到图像的映射：** 要让余弦波到图像的映射更合理可以把像素值平移128个单位`(0,255)->(-128,127)`此时，像素变化的大小和方向便反映到原始余弦波的振幅上即`DCT`的系数。\n\n![image-20220519172509224](imagecompress/image-20220519172509224.png)\n\n**还有什么变动会影响DCT呢？**比较简单的是单纯把波整体上下平移，**==看起来升降信号只影响系数X0==**。当提高频率时，发现有多个不同的`DCT`系数与余弦波相对应，当变为`cos(2x)`后只有系数X2不为0，这个余弦波只是将之前哪个余弦波的频率翻倍了。频率为0的余弦波是一个常值信号提供了标准，去衡量**一组像素的整体亮度**\n\n![image-20220519202319099](imagecompress/image-20220519202319099.png)\n\n每个频率都对应一个不同的图像模式，`DCT`的核心是分解出各个基本模式，分析它们对原始图像的贡献，结果发现八个像素的所有可能组合都可表示为这八个余弦波的总和\n\n![image-20220519202612699](imagecompress/image-20220519202612699.png)\n\n`DCT`的数学定义如上，其中`k`为余弦波的频率，原始信号点可以表示为一个矢量，我们可以将余弦波的采样点也改写成矢量\n\n![image-20220519202933577](imagecompress/image-20220519202933577.png)\n\n向量点积的形式是衡量两者相似性的好方法，这解释了当我们对频率为`k`的余弦波采样，将其输入`DCT`后位于`k`号位的系数为何会出现尖峰。这两个向量互为倍数，所以点积被最大化了\n\n![image-20220519204120404](imagecompress/image-20220519204120404.png)\n\n我们可以把整个`DCT`看成是矩阵与向量之积，矩阵每行是各自频率的余弦波中的采样点，**==令人震惊的是矩阵中每行都是相互正交的==**，这就是为什么直接只输入一个特定频率的余弦波时，其他系数没有任何贡献（来自不同余弦波的采样点相互正交 ）\n\n> 扩展：DFT中扩展序列时，是直接采用平移方式引入了不连续的区间，而DCT采用对称形式，消除了这一人为的不连续（扩展后的函数在该点变化非常快，信号分解过程中会出现多个高频信号），而通常高频信号隐藏在这人造的不连续中。这也正是为什么DCT的能量聚集效果会更好。\n\n### JPEG如何具体使用DCT\n\n![image-20220519205438515](imagecompress/image-20220519205438515.png)\n\n首先将图片分为多个8*8的区域，各个元素减去128使值域中心为0，再对这个矩阵的行列进行`DCT`变换，各系数分别代表某个8 * 8图案的权重，其他格子都是第一行和第一列这些基础图案的组合\n\n![image-20220519221457268](imagecompress/image-20220519221457268.png)\n\n只加入小部分的频率，我们的信号和图像也和原图相差无几了\n\n![image-20220519221911043](imagecompress/image-20220519221911043.png)\n\n最大系数在左上角也就是低频部分，有趣的是图里的各个8*8矩阵基本都满足这个性质，这个性质称为能量集中（图像压缩领域的重要概念）。进行变化后，其中最大的那些值会集中在几个低频系数上，复原图像时每个小块只用一部分的系数（**最低频的分量**）肉眼就很难分辨了，正是这个概念让我们在不降低观感的同时能够高度压缩图片。随着频率的提高，人对高频系数逐渐不敏感，就可以丢弃`DCT`的高频分量，如何做呢？\n\n### DCT如何丢弃高频分量\n\n![image-20220520093829648](imagecompress/image-20220520093829648.png)\n\n==量化：== 给定一个8*8矩阵代表`DCT`输出的频率系数，将每个元素分别除以一个标量值（有不同的量化表分别用于亮度通道和色彩通道），然后取整。\n\n![image-20220520094009730](imagecompress/image-20220520094009730.png)\n\n解码阶段不太重要的高频分量会出现一大堆0，完成量化后，可以利用冗余来进一步压缩，这涉及到`游程编码和霍夫曼编码的组合`。\n\n![image-20220520095419761](imagecompress/image-20220520095419761.png)\n\n编码将系数按之字形排列，让序列尽量多出现连续0，然后用游程编码压缩，霍夫曼编码较复杂此处省略具体操作。\n\n## 总结\n\nJPEG中的许多革新时源于人类视觉系统的实验和理解，这让我们知道人眼对色彩和高频信息并不敏感，音频视频压缩上也可同样的技术应用。\n\n> [视频：JPEG不可思议的压缩率](https://www.bilibili.com/video/BV1iv4y1N7sq?spm_id_from=444.41.list.card_archive.click)\n","tags":["image processing"],"categories":["course"]},{"title":"Swin","url":"/2022/05/17/swin/","content":"\n> 对`霹雳吧啦Wz`swin transformer的代码总结\n\n# 代码结构\n\n`Swin Transformer` ------`PatchEmbed`\n\n​\t\t   \t\t\t\t\t\t\t\t`PatchMerging`                                    \n\n​\t\t\t\t\t\t\t\t\t\t\t`BasicLayer` ------`create_mask`------`window_partition and window-reverse`\n\n​                                            `Swin Transformer Block` ------`Window attention`\n\n## 1. patchEmbed\n\n```python\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D Image to Patch Embedding\n    \"\"\"\n    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        patch_size = (patch_size, patch_size)\n        self.patch_size = patch_size\n        self.in_chans = in_c\n        self.embed_dim = embed_dim\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        _, _, H, W = x.shape\n\n        # padding\n        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding\n        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n        if pad_input:\n            # to pad the last 3 dimensions,\n            # (W_left, W_right, H_top,H_bottom, C_front, C_back)\n            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n                          0, self.patch_size[0] - H % self.patch_size[0],\n                          0, 0))\n\n        # 下采样patch_size倍\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        # flatten: [B, C, H, W] -> [B, C, HW]\n        # transpose: [B, C, HW] -> [B, HW, C]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n```\n\n## 2. PatchMerging\n\n```python\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    # 传入前面保存好的H,W\n    def forward(self, x, H, W):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        x = x.view(B, H, W, C)\n\n        # padding\n        # 如果输入feature map的H，W不是2的整数倍，需要进行padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            # to pad the last 3 dimensions, starting from the last dimension and moving forward.\n            # (C_front, C_back, W_left, W_right, H_top, H_bottom)\n            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同\n            # 此处与1中pad方式不同？\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]\n        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]\n\n        x = self.norm(x)\n        x = self.reduction(x)  # [B, H/2*W/2, 2*C]\n        return x\n```\n\n## 3. BasicLayer\n\n```python\n#---------------SwinTransformer类下构建了BasicLayer-------------------------\n# build layers\nself.layers = nn.ModuleList()\nfor i_layer in range(self.num_layers):\n    # 注意这里构建的stage和论文图中有些差异\n    # 这里的stage不包含该stage的patch_merging层，包含的是下个stage的，构建前三个stage要下采样\n    layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                        depth=depths[i_layer],\n                        num_heads=num_heads[i_layer],\n                        window_size=window_size,\n                        mlp_ratio=self.mlp_ratio,\n                        qkv_bias=qkv_bias,\n                        drop=drop_rate,\n                        attn_drop=attn_drop_rate,\n                        drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                        norm_layer=norm_layer,\n                        downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                        use_checkpoint=use_checkpoint)\n    self.layers.append(layers)\n    \n# 整体前向过程\n    def forward(self, x):\n        # x: [B, L, C]\n        x, H, W = self.patch_embed(x)\n        x = self.pos_drop(x)\n\t\t\n        # 每个Stage中都将当前的H，W传入，并保存经过当前stage后的H,W\n        for layer in self.layers:\n            x, H, W = layer(x, H, W)\n\n        x = self.norm(x)  # [B, L, C]\n        x = self.avgpool(x.transpose(1, 2))  # [B, C, 1]\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n```\n\n```python\n#---------------------------BasicLayer类中构建每个stage---------------------------\n# build blocks\nself.blocks = nn.ModuleList([\n    SwinTransformerBlock(\n        dim=dim,\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0 if (i % 2 == 0) else self.shift_size,\n        mlp_ratio=mlp_ratio,\n        qkv_bias=qkv_bias,\n        drop=drop,\n        attn_drop=attn_drop,\n        drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n        norm_layer=norm_layer)\n    for i in range(depth)])\n\n    def create_mask(self, x, H, W):\n        # calculate attention mask for SW-MSA\n        # 保证Hp和Wp是window_size的整数倍\n        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n        # 拥有和feature map一样的通道排列顺序，方便后续window_partition\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]\n        h_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        w_slices = (slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None))\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]\n        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]\n        # 每一个窗口中的值展平后复制9次，与所有值复制9次后做差，相邻区域填0\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1] \n        # [nW, Mh*Mw, Mh*Mw]\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        return attn_mask\n    \n#该类的前传过程，每个stage后都会返回H,W\n    def forward(self, x, H, W):\n        attn_mask = self.create_mask(x, H, W)  # [nW, Mh*Mw, Mh*Mw]\n        for blk in self.blocks:\n            # 添加H,W类属性\n            blk.H, blk.W = H, W\n            if not torch.jit.is_scripting() and self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, attn_mask)\n            else:\n                x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x = self.downsample(x, H, W)\n            # +1：防止为奇数后续操作需要填充\n            H, W = (H + 1) // 2, (W + 1) // 2\n\n        return x, H, W\n```\n\n```python\n#-------------------------------- SwinTransformerBlock的具体实现-------------------------------\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,\n            attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, attn_mask):\n        # 在遍历self.blocks的过程中会将H,W复制给类变量，所以此处可以取到H,W\n        H, W = self.H, self.W\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # pad feature maps to multiples of window size\n        # 把feature map给pad到window size的整数倍\n        pad_l = pad_t = 0\n        pad_r = (self.window_size - W % self.window_size) % self.window_size\n        pad_b = (self.window_size - H % self.window_size) % self.window_size\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n        _, Hp, Wp, _ = x.shape\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n            attn_mask = None\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # [nW*B, Mh, Mw, C]\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [nW*B, Mh*Mw, C]\n\n        # W-MSA/SW-MSA,传入了填充后的H,W\n        attn_windows = self.attn(x_windows, Hp, Wp, mask=attn_mask)  # [nW*B, Mh*Mw, C]\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)  # [nW*B, Mh, Mw, C]\n        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # [B, H', W', C]\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n\n        if pad_r > 0 or pad_b > 0:\n            # 把前面pad的数据移除掉\n            x = x[:, :H, :W, :].contiguous()\n\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n```\n\n```PYTHON\n# 包含相对位置编码部分\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # [Mh, Mw]\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # [2, Mh, Mw]\n        coords_flatten = torch.flatten(coords, 1)  # [2, Mh*Mw]\n        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, Mh*Mw, Mh*Mw]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # [Mh*Mw, Mh*Mw, 2]\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # [Mh*Mw, Mh*Mw]\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, Mh*Mw, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        # [batch_size*num_windows, Mh*Mw, total_embed_dim]\n        B_, N, C = x.shape\n        # qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]\n        # reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]\n        # permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        # transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]\n        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        # relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Mh*Mw, Mh*Mw]\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            # mask: [nW, Mh*Mw, Mh*Mw]\n            nW = mask.shape[0]  # num_windows\n            # attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]\n            # mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n        # transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]\n        # reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n```\n\nLINE247 `self.attn(x_windows, H, W, mask=attn_mask)`加入了H,W\n\nLINE57 新增卷积分支\n\nLINE81 `forward`中传入H,W\n\nLINE89-105 更新V\n\n","tags":["code"],"categories":["source code"]},{"title":"A White-box Deep Network from the Principle of Maximizing Rate Reduction","url":"/2022/05/15/WhiteboxNet/","content":"\n> What I cannot create, I do not understand. - Richard Feynman\n\n`马毅介绍了近期的工作：通过优化 MCR^2 目标，能够直接构造出一种与常用神经网络架构相似的白盒深度模型，其中包括矩阵参数、非线性层、归一化与残差连接，甚至在引入「群不变性」后，可以直接推导出多通道卷积的结构。该网络的计算具有精确直观的解释，受到广泛关注。这个框架不仅为理解和解释现代深度网络提供了新的视角，有可能地改变和改进深度网络的实践所得到的网络将完全是一个“白盒”，而随机初始化的反向传播不再是训练网络的唯一选择。`\n\n# 0. High-Dim Data\n\n![image-20220515135836762](WhiteboxNet/1.png)\n\n每一类的数据一定在高维空间中的一个低维的流形或者分布上，实际上整个机器学习就是在学到底谁是谁，找到这些结构。所有目前基于数据的AI或者机器学习都在做以下三件事情：\n\n> 1. 数据插值——寻找样本之间的相似关系，这体现为聚类或分类任务\n> 2. 在上一步表现良好的基础上，可以判断新的数据所属类别\n> 3. 当对以上两点理解不错的时候，可以考虑将数据表达的更好\n\n![image-20220515141128956](WhiteboxNet/image-20220515141128956.png)\n\n整个网络学习过程中我们并不知道数据内在的结构是什么\n\n![image-20220515141330628](WhiteboxNet/image-20220515141330628.png)\n\n很多理论希望去解释深度学习到底在干什么，其中一种是说：深度网络其实是从数据中抓取一些与标签最相关的特征，同时把不相干的特征扔掉，与具体任务强相关不通用。\n\n![image-20220515142430195](WhiteboxNet/image-20220515142430195.png)\n\n> 1. 从计算的角度来说，在处理高维数据的时候，传统信息论的统计量是无法定义的，高维数据常常是退化分布的，无法完成有效测量。\n> 2. 当数据有低维结构时，这些量在数学上甚至没有定义\n\n很多NIPS文章提出一个理论，紧接着马上开始近似，某些量是independence开始计算，两三步后与原始信息的区别和联系都不知道了，如何提供指导。\n\n##  1. 区分数据\n\n![image-20220515151501291](WhiteboxNet/image-20220515151501291.png)\n\n![image-20220515144328888](WhiteboxNet/image-20220515144328888.png)\n\n传统聚类方法通常采用最大化相似度的方法进行，而应用在高维退化分布的数据上时，相似度难以定义。因此，我们从更基础的问题出发，为什么需要聚类划分数据？\n\n从压缩角度，我们可以看出，能够划分的数据具有更小的空间，通过划分能够获得对数据更有效的表示。如果能找到编码长度的有效度量，就可以设计相应的优化目标。\n\n熵是度量编码长度的工具，但在高维数据上，熵的测量非常困难，马毅教授采用率失真理论来度量这样的表示，提出了编码长度函数（Coding Length Function）\n\n![image-20220515144801577](WhiteboxNet/image-20220515144801577.png)\n\n上述公式相当于给一组数据就输出其需要的bits存储空间\n\n有上图的度量后，我们就能描述聚类或划分的现象，即划分前的数据所须的编码长度，大于划分后的编码长度。这样的划分不需要标签，而是可以通过一些贪心算法，比较不同划分之间的编码长度，获得使划分后编码长度最小的划分。结果展现了这样的方法有非常好的聚类效果，能够找到全局最优的划分，并对离群点非常鲁棒。\n\n![image-20220515145951032](WhiteboxNet/image-20220515145951032.png)\n\n整个的数据分类就变成一个压缩的问题，要分开的话，其分开后的压缩的量用的bits最少\n\n![image-20220515150154121](WhiteboxNet/image-20220515150154121.png)\n\n这个量非常神奇，你可以把数据每个点都分开，再两两融合（如果可以省bits），可以得到一个`pwm`算法 \n\n![image-20220515150847050](WhiteboxNet/image-20220515150847050.png)\n\n神奇在数据有很多的噪声和异常值的情况下，都能找到低维结构\n\n![image-20220515151029152](WhiteboxNet/image-20220515151029152.png)\n\n仅使用数据压缩的算法，就取得了当时最好的分割效果\n\n## 2. 划分新数据\n\n![image-20220515151407157](WhiteboxNet/image-20220515151407157.png)\n\n当数据有低维结构时此公式不适用，教课书中教的是上图公式的做法，但实际应用中并不用统计教科书中的方式。\n\n![image-20220515151907078](WhiteboxNet/image-20220515151907078.png)\n\n不适用应该怎么办呢最大似然估计不work，又回到编码量。如果要对某个样本分类，那么包含其后那一类相比其他类的编码量应该是最小的。\n\n同样的方法可以应用于分类任务，通过比较将新数据划分到不同类别增加的编码长度，选取使编码长度增加最少的类别，作为该样本最合适的分类，这种方法依旧来源于最小划分后编码长度的理论。这种方法可以理解为，将新样本划分到合适的类别分类后，所带来的存储开销应当最少，通过正确分类，可以得到最优的表示效率。结果显示，比较传统方法，MICL能够找到更加紧的边界，并且与分类不同的是，其决策边界更接近于数据本身的结构特征。\n\n![image-20220515152725594](WhiteboxNet/image-20220515152725594.png)\n\n整个过程可以通过数学证明\n\n![image-20220515152934139](WhiteboxNet/image-20220515152934139.png)\n\n通过简单的分类，已经学到了数据的低维结构，但只是对低维子空间和高斯分布很准，当数据有非线性结构不适用。\n\n##  3. 表征数据\n\n`在完成了 Interpolation(聚类)与 Extrapolation(分类)后，从压缩的视角，还能够实现对数据的表示。当数据符合某种低秩结构时，优秀的表达的目标可以被理解为，最大限度地学习到该结构特征，即，在让同一结构样本靠近的同时，使样本表达能力最大；同时，将不同结构数据间的差异尽可能清晰地体现出来。`\n\n![image-20220515153419946](WhiteboxNet/image-20220515153419946.png)\n\n![image-20220515153857958](WhiteboxNet/image-20220515153857958.png)\n\n![image-20220515154142217](WhiteboxNet/image-20220515154142217.png)\n\n如何判断变换后特征的好坏呢？通过衡量整体和局部平均编码量来衡量\n\n![image-20220515154630286](WhiteboxNet/image-20220515154630286.png)\n\n![image-20220515154901992](WhiteboxNet/image-20220515154901992.png)\n\n同类（局部）越近越好，不同类（整体）越远越好\n\n![image-20220515160633316](WhiteboxNet/image-20220515160633316.png)\n\n左优于右，蓝色球数量多。为了使不同范围的样本进行比较，针对每个样本需要进行归一化操作。这与归一化的通常理解相符，使模型能够比较不同范围的样本。\n\n![image-20220515194314867](WhiteboxNet/image-20220515194314867.png)\n\n在宽泛的条件下，数学上可以证明当特征达到最大编码量的时候，可以证明每类的特征，彼此是正交的；每类的特征可以把子空间占满。\n\n## 4. 实验（MCR取代Cross Entropy）\n\n![image-20220515195017779](WhiteboxNet/image-20220515195017779.png)\n\n训练过程中的三个量是在物理、几何和统计上意义严格的量，整体的Vol在增长，局部的Vol在压缩，差值在增长。\n\n![image-20220515195514951](WhiteboxNet/image-20220515195514951.png)\n\n不同类的特征完全正交，每一类的特征均匀的分布在各个子空间上。\n\n![image-20220515195843123](WhiteboxNet/image-20220515195843123.png)\n\n目标函数不再拟合label，从整体去学习特征，鲁棒性很强\n\n## 5. 本质是什么\n\n![image-20220515200105407](WhiteboxNet/image-20220515200105407.png)\n\n![image-20220515200746850](WhiteboxNet/image-20220515200746850.png)\n\n这是个非凸的问题，我们的目的是研究什么样的Z能优化这个问题，不管是deep learning或是其他方式。神经网络告诉我们当遇到一个很难的问题时，其他做不了可以做梯度下降，训练一下调一调。\n\n![image-20220515201237687](WhiteboxNet/image-20220515201237687.png)\n\n对该目标求梯度后，获得了两个操作矩阵E、C，所求梯度就是其分别与样本乘积的和。\n\n![image-20220515201743335](WhiteboxNet/image-20220515201743335.png)\n\n而观察E、C两个操作矩阵，会发现其与样本乘积的结果天然带有几何的解释，即矩阵作用于所有的样本数据上，和其他的矩阵作用于不同类的数据上。当使用其他的数据对每个数据做回归得到的残差，就是\n\n因此，若需要扩展样本空间的大小，只需加上E与样本相乘获得的残差，若要压缩各类别子空间的大小，仅需减去与C进行相同操作的结果。\n\n![image-20220515204644473](WhiteboxNet/image-20220515204644473.png)\n\n![image-20220515204808657](WhiteboxNet/image-20220515204808657.png)\n\n每个EC、CZ都是自回归的残差。对比常 用的神经网络结构，可以发现其与`ReduNet`有许多相似之处，例如残差链接，C的多通道性质，非线性层等。同时，`ReduNet`所有参数均能够在前向传播中计算得到，因此网络无需BP优化。\n\n![image-20220516144746606](WhiteboxNet/image-20220516144746606.png)\n\n通过引入组不变性，将cyclic shift后的样本视为同一组，每次将一组样本编码到不同低秩空间，`ReduNet`可以实现识别的平移不变性。同时，类似卷积的网络性质也随之而来。在引入平移不变的任务要求后，网络使用循环矩阵表示样本，因而在与E，C矩阵进行矩阵乘时，网络的操作自然地等价于循环卷积。在压缩数据的过程中，梯度下降中的算子自动变成卷积。\n\n![image-20220516150617561](WhiteboxNet/image-20220516150617561.png)\n\n求逆计算使得通道间的操作相互关联。上述计算还可以通过频域变换来加速计算效率。\n\n![image-20220516151156033](WhiteboxNet/image-20220516151156033.png)\n\n整个优化过程中自然的出现了神经网络中的提出的种种算子。尽管上文中算法有诸多变化，其核心都是基于“压缩”的概念。 聚类，划分，表征，这些学习任务都可以被表述成压缩任务。我们希望学习到样本的知识，是期望能够更高效地表示样本，因此我们学习类别，提取特征，抽象概念。 MCR^2 原理基于率失真理论，描述了划分和压缩的过程，并能够基于压缩，完成包括聚类，分类，表示学习，构造网络等等任务，体现了作为学习的一般原理的泛用性能。\n\n![image-20220516153251893](WhiteboxNet/image-20220516153251893.png)\n\n**QA:** \n\n每个神经元可能是原始高维空间中的一个切割平面 ，新来的数据和切割平面去比到底在切割平面的哪一边，一个神经元的输出可以得出属于哪一边，相当于encoding；可以在这个高维空间中一直切，切到一定的程度，对于任何input都可以encoding，可以基本知道这个点在高维空间中的位置。神经元的数学模型可能就是高维空间中的切割平面，用来做encoding，每一层的个数相当于数据的内在结构的维度。第二层的神经元相当于使近的更近，远的更远，后续层相当于优化迭代的次数。网络是前向学习的，即使只有少量样本就能学到很好的参数，学习过程中不用大量的资源。\n\n![image-20220516161208731](WhiteboxNet/image-20220516161208731.png)\n\n> - [其他总结](https://www.cn-healthcare.com/articlewm/20210428/content-1214792.html)\n>\n> - [视频讲座](https://www.bilibili.com/video/BV1LB4y1u79M?spm_id_from=333.788.b_636f6d6d656e74.5)\n","tags":["paper"]},{"title":"Pytorch","url":"/2022/04/25/pytorch/","content":"\n# 1. Module类  \n\n- module中的__getattr中有三个魔法函数，其有三个成员变量分别是`_parameters`、`_buffers`(统计量)和`_modules`\n\n```python\n# 实例化模型后，参数已初始化\ntest_module = Net()\n# 返回模型中各个模块层（不包含子层）,返回的是有序字典，与named_children()区别，其返回的是元组\ntest_module._modules\n# 打印模块自身的Parameters、buff对象，不包括其中各个子模块\ntest_module._parameters\ntest_module._buffers\n\n# named_modules()不仅返回自身模块还返回各个子模块 \nfor p in test_module.named_modules():\n    print(p)\n```\n\n```python\n# 补充：model.modules(), model.children(), model.named_children(), model.parameters()，model.state_dict()\n# 作用及区别\nimport torch \nimport torch.nn as nn \n \nclass Net(nn.Module):\n \n    def __init__(self, num_class=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3),\n            nn.BatchNorm2d(6),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=6, out_channels=9, kernel_size=3),\n            nn.BatchNorm2d(9),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n \n        self.classifier = nn.Sequential(\n            nn.Linear(9*8*8, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(128, num_class)\n        )\n \n    def forward(self, x):\n        output = self.features(x)\n        output = output.view(output.size()[0], -1)\n        output = self.classifier(output)\n    \n        return output\n \nmodel = Net()\n\n# 1. model.modules()\n#    迭代遍历模型的所有子层，在本文的例子中，Net(), features(), classifier()\n# \t 以及nn.xxx构成的卷积，池化，ReLU, Linear, BN, Dropout\n# \t 也就是model.modules()会迭代的遍历模型的所有子层   len(model_modules)： 15\nmodel_modules = [x for x in model.modules()]\nprint(model_modules)\n\nfor layer in model.modules():\n    if isinstance(layer, nn.Conv2d):\n        pass\n        \n# 2. model.named_modules()\n# \t 不但返回模型的所有子层，还会返回这些层的名字\n#    ('features.0', Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)))\nmodel_named_modules = [x for x in model.named_modules()]   \nprint(model_named_modules) \n\n# 返回层以及层的名字的好处是可以按名字通过迭代的方法修改特定的层\nfor name, layer in model.named_modules():\n    if 'conv' in name:\n        pass\n \n# 3. model.children()  \n#    只遍历模型的子层，这里即是features和classifier，len(model_children)： 2\nmodel_children = [x for x in model.children()]    \nprint(model_children) # [Sequential(), Sequential()]\n# 修改特定层\nmodel.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n\n# 4. model.named_children()\n# \t 不但迭代的遍历模型的子层，还会返回子层的名字\nmodel_named_children = [x for x in model.named_children()]  \nprint(model_named_children) # [('features',Sequential()), ('classifier',Sequential())]\n\n# 5. model.parameters()、model.named_parameters()\n# \t len(model_named_parameters)：12     \nmodel_named_parameters = [x for x in model.named_parameters()]\nprint(model_named_parameters)\n\n#  model.state_dict()\n#  返回一个有序字典，下例为使用预训练例子\npretrained_dict = torch.load(log_dir)  # 加载参数字典\nmodel_state_dict = model.state_dict()  # 加载模型当前状态字典\npretrained_dict_1 = {k:v for k,v in pretrained_dict.items() if k in model_state_dict}  # 过滤出模型当前状态字典中没有的键值对\nmodel_state_dict.update(pretrained_dict_1)  # 用筛选出的参数键值对更新model_state_dict变量\nmodel.load_state_dict(model_state_dict)  # 将筛选出的参数键值对加载到模型当前状态字典中\n```\n\n- module中的`state_dict`方法\n  1. 首先，通过`_save_to_state_dict`方法先将当前模块的`parameters`和`buffers`变量存入`destination`字典\n  2. 然后，遍历`self.modules.items()`子模块，将每个子模块的`parameters`和`buffers`变量存入`destination`字典\n  3. 最后返回`destination`字典中包含所有模型状态参数，OrderedDict[key, value]\n\n```python\n>>> module.state_dict().keys()\n['bias', 'weight']\n\n# 将网络参数保存到path文件中，不包含网络图结构和优化器参数等部分\ntorch.save(test_module.state_dict(), path)\n\n#———————————————————————————模型保存相关—————————————————————\n# 保存网络参数和图结构，占用磁盘空间大\ntorch.save(test_module, path)\n\n# 保存所有所有相关信息\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\ntorch.save({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)\n\n# 再次实例化，是因为上面torch.save中没有保存网络图结构，要先构建图结构再去加载参数\nmodel = Net()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = torch.load(PATH)\n# 传入字典对象\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n\n```\n\n- module中的`parameter`方法\n\n  1. 注意`parameter` 与`_parameters `区分，前者返回的是迭代器，包括当前模块和各个子模块的参数。后者返回的是当前模块中的参数。\n  2. 其中会执行`named_papameters` 最终返回的是个迭代器对象[key, value]\n\n  ```python\n  >>> for p in test_module.named_parameters():\n      \tprint(p)\n  ```\n\n- module中的`train` 方法\n\n  `dropout` 和 `batchnorm` 都继承了Module类，都属于模型的子模块，当把模型设置为训练模式和验证模式时，相应的子模块也会设为对应的训练或验证模式\n\n# 2. 自动微分Forward与Reverse模式\n\n- ## Forward计算流程\n\n![forward](pytorch/image-20220426135242639.png)\n\n**==特点：==** 前向计算过程中当前节点相对某个输入结点的梯度可计算得到；每次只能得到一个输入节点的导数如图中的x1\n\n- ## Reverse计算流程\n\n![reverse](pytorch/image-20220426135907575.png)\n\n**==特点：==** 反向计算过程中需要等待前向计算结束；一次性可以算出所有节点导数 \n\n> 图片出处：Automatic Differentiation in Machine Learning: a Survey\n\n# 3. 算子融合\n\n```python\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nin_channels = 2\nout_channels =2\nkernel_size = 3\nw = 9\nh = 9\n\nx = torch.ones(1, in_channels, h, w)\nconv3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconv1 = nn.Conv2d(in_channels, out_channels, 1)    # [2,2,1,1]\nresult1 = x + conv3(x) + conv1(x)\nprint(result1)\n\n# ——————————————————————--————将1*1卷积转为3*3——————————————————————————————————————\n\n# pad填充方式从里到外，每个维度都有上下和左右两个方向，四个1分别对应左右S上下填充0的个数\n# [2,2,1,1] -> [2,2,3,3]\nconv1_to_conv3 = F.pad(conv1.weight, [1,1,1,1,0,0,0,0]) \n\n# 实例化卷积\nconv1_3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\n\n# 将卷积参数用1*1填充后的参数替代，weight是parameter类，要用nn.Parameter()包装\nconv1_3.weight = nn.Parameter(conv1_to_conv3)\nconv1_3.bias = conv1.bias\n\n#--------------------------------如何将输入x本身化为3*3卷积表示—————————————————————————————————\n#------------------------1. 必须是1*1的卷积不考虑相邻点融合--------------—————————————————————\n#-------------------—2. 不考虑通道间的融合（只有一个通道中含有非0数）----------------------------\n\n# zeros:不考虑通道之间影响\tchannel:不考虑相邻点影响  \tweight[2,2,3,3]：共四个3*3矩阵\nzeros = torch.unsqueeze(torch.zeros(kernel_size, kernel_size), 0)\nchannels = torch.unsqueeze(F.pad(torch.ones(1, 1), [1,1,1,1]), 0)\n\n# 对应第一个通道卷积核\nchannel_zeros = torch.unsqueeze(torch.cat([channels, zeros], 0), 0)\n\n# 对应第二个通道卷积核\nzeros_ channel= torch.unsqueeze(torch.cat([zeros, channels], 0), 0)\n\nidentity_conv_weight = torch.cat([channel_zeros, zeros_ channel], 0)\nidentity_conv_bias = torch.zeros([out_channels])\n\nconvx_3 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconvx_3.weight = nn.Parameter(identity_conv_weight)\nconvx_3.bias = nn.Parameter(identity_conv_bias)\n\nresult2 = conv3(x) + conv1_3 + convx_3\nprint(result2)\n\n#--------------------------------融合—————————————————————————————————\nconv_fusion = nn.Conv2d(in_channels, out_channels, kernel_size, padding=\"same\")\nconv_fusion.weight = nn.Parameter(conv3.weight.data + conv1_3.weight.data + convx_3.weight.data)\nconv_fusion.bias = nn.Parameter(conv3.bias.data + conv1_3.bias.data + convx_3.bias.data)\nresult3 = conv_fusion(x)\nprint(torch.all(torch.isclose(result2, result3)))\n```\n\n# 4. Hooks机制\n\n> Pytorch提供的hooks机制能让用户可以往计算流中的某些部分注入代码，一般来说这些部分无法直接从外部访问。其中主要有两种hooks，一种是添加到张量上的hooks，另一种是添加到Module上的hooks。\n\n## 4.1 添加到张量上的hooks\n\n这些添加的`hooks`能够让用户在反向传播的过程中访问到计算图中的梯度。\n\n下面先来看看反向传播的一个具体例子。\n\n![image-20220523195647470](pytorch/image-20220523195647470.png)\n\n- 当我们将张量`a`和`b`相乘的同时也在构建后向图，即创建了一个名字是`MulBackward0`的节点(其中`next_functions`表示梯度接下来要传递的到哪些节点即操作的输入)，还有两个`AccumulateGrad`节点(将反向传播过程中对应张量的梯度作累加)。最后得到的梯度值将保存到叶子张量上(绿框)。\n\n- `张量c`属于中间节点，其中`grad_fn`属性指向后向图中的`MulBackward0`节点，`c.backward()`就是对应这个过程将起始梯度传给`MulBackward0`节点。然后再将该梯度传给`MulBackward0`节点中的`backward`函数(本质就是将输入梯度乘对应值得到输入张量的梯度)。本例中前传`a*3`和`2*b`所以此节点对应张量`a`和`b`的梯度为3，2。\n\n  最后要将输出梯度1分别乘3，2得到输入梯度，然后传递给`a`和`b`的`AccumulateGrad`节点来累加梯度，该节点将最终的梯度赋值给对应张量的`grad`属性。\n\n以上整个过程一旦调用了`.backward()`反向传播过程中中间节点所产生的梯度(红框)都是无法访问到的，无法打印、修改，用户只能查看反向回传完之后叶子节点上的梯度。\n\n**hooks的作用在于能够让用户访问到反向传播过程中的梯度张量，同时可以修改这些梯度值。**\n\n------\n\n我们给中间的张量都添加`hooks`看看计算图会有什么变化\n\n![image-20220523204413424](pytorch/image-20220523204413424.png)\n\n- 第一个添加的`hook`: `c.register_hook`中传入了一个函数`c_hook`,该函数有一个参数表示梯度，并可以返回一个新梯度。当向张量`c`注册这个`hook`函数，首先它会被添加到张量`c`的`_backward_hooks`（是个有序字典，添加`hook`函数的顺序很重要，反向传播中会按照之前添加的顺序调用）。\n\n- 如果用户想让梯度保存在某个中间节点的话，需要调用`中间节点的retain_grad函数`(默认情况下，只有叶子节点会保存梯度值)。在调用此函数后，会往`_backward_hooks`字典中注册`retain_grad_hook`函数，当该函数被调用，传给它的梯度值会保存到中间张量的`grad`属性上。\n\n- **需要注意的是反向传播过程中`hook`系统是如何工作的，往中间节点和叶子节点上添加`hook`是有区别的。**当往叶子节点添加`hook`函数，该函数就只是被添加到`_backward_hooks`字典中。而往中间节点添加`hook`函数的同时，所有在反向图中关联了该中间向量的节点都会被通知，上图中`MulBackward0`节点关联了张量`c`，即将`_backward_hooks`字典添加到`MulBackward0`节点的`pre_hooks`列表中，这些列表中函数都会在梯度被传递给`backward`函数前被调用。\n\n  在注册好所有hook函数后，过一遍反向传播过程。\n\n![image-20220523220453700](pytorch/image-20220523220453700.png)\n\n流程：`1.0`->`MulBackward0`->`pre_hooks`->`_backward_hooks`->将梯度2->`backward`->8,12(12会被传递给叶子张量d的`AccumulateGrad`节点，**同时该节点会检查其所关联的张量是否有注册**`backward_hooks`，如果注册了（`d`中注册了，`a,b`未注册），该节点会把梯度传给这些注册的`hook`函数处理，然后再保存到张量的`grad`属性上)->梯度10->`backward`-> ....\n\n![image-20220523221008946](pytorch/image-20220523221008946.png)\n\n`h.remove()`可以将`hook`函数从保存它的`_backward_hooks`字典中移除，如上图在调用`e.backward`之前调用了`h.remove`，那么在反向传播中这个`c_hook`函数就不会被调用，另外要注意的是，在这些`hooks`函数中不要对梯度张量本身做任何修改，即不要对输入梯度做`inplace`操作如`grad *= 100`。原因是这个梯度有可能同时被传递给后向图中其他节点。\n\n## 4.2 Module上的hooks\n\n- `Module`上的`hooks`函数是在`forward`函数调用之前或之后被调用的。下面例子实现了一个`SumNet`模块，其`forward`函数是将三个张量相加并返回结果。\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass SumNet(nn.Module):\n    def __init__(self):\n        super(SumNet，self)._.init__()\n        \n    @staticmethod\n    def forward(a，b，c):\n        d = a + b + c\n        return d\n\ndef forward_pre_hook(module，inputs):\n    a，b = inputs\n    return a + 10，b\n\n#此处inputs参数是forward_pre_hook函数的返回值input(11,2),output的输出会覆盖forward函数中返回的输出，即116\ndef forward_hook(module，inputs， output):\n    return output + 100\n\ndef main():\n    sum_net = SumNet()\n    \n    # 往模块注册在forward之前调用的hook函数，执行完该函数后，会将更新后的a=11, b=2, c=3输入forward函数，并返回d=16\n    sum_net.register_forward_pre_hook(forward_pre_hook)\n    #注册在forward之后调用的hook函数\n    sum_net.register_forward_hook( forward_hook)\n    \n    a = torch.tensor(1.0，requires_grad=True)\n    b = torch.tensor(2.0，requires_grad=True)\n    c = torch.tensor(3.0，requires_grad=True)\n    \n    #a, b作为位置参数传入，c作为 关键字参数传入\n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#116\n```\n\n- 和往张量上注册`hooks`函数一样，同样可以用一个变量来保存注册`hooks`函数时的返回值，即`hook`函数的句柄（handle to the hook），这样方便后面移除`hook`。\n\n```python\ndef main():\n    sum_net = SumNet()\n    \n\tforward_pre_hook_handle = sum_net.register_forward_pre_hook (forward_pre_hook)\n\tforward_hook_handle = sum_net.register_forward_hook(forward_hook)\n\t\n    a = torch.tensor(1.0，requires_grad=True)\n    b = torch.tensor(2.0，requires_grad=True)\n    c = torch.tensor(3.0，requires_grad=True)\n    \n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#116\n    \n\tforward_pre_hook_handle.remove()\n\tforward_hook_handle.remove()\n    \n    d = sum_net(a，b，c=c)\n    print( 'd: ', d)\t\t#6\n```\n\n- `Module`还有另一种`hook`，即`backward_hook`。`register_backward_hook(backward_hook)`\n\n  ```\n  # args：实例、输入梯度、输出梯度\n  def backward_hook(module，grad_input，grad_output):\n  \tprint('module:', module)\n  \tprint('grad_input:',grad_input)\n  \tprint('grad_output:',grad_output)\n  ```\n\n  > [PyTorch Hooks Explained - In-depth Tutorial](https://www.bilibili.com/video/BV1MV411t7td?spm_id_from=333.337.search-card.all.click)\n\n# 5. einsum\n\n基本规则：\n\n- 在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作。如`\"ik,kj->ij\"`，`k` 在输入中重复出现，所以就是把 `a` 和 `b` 沿着` k` 这个维度作相乘操作\n- 只出现在一边的索引，表示中间计算结果需要在这个维度上求和\n- 等式 右边的索引顺序可以是任意的，比如上面的` \"ik,kj->ij\" `如果写成` \"ik,kj->ji\"`，那么就是返回输出结果的转置，用户只需要定义好索引的顺序。\n\n特殊规则：\n\n- 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 \"ik,kj->ij\" 也可以简化为 \"ik,kj\"。\n- 支持` \"...\" `省略号，用于表示用户并不关心的索引。\n\n```python\nimport torch\n\na = torch.rand(2)\nb = torch.rand(3)\n\n# Out product\nout = torch.einsum(\"i,j->ij\", a, b)\n\nx = torch.rand((2, 3))\nv = torch.rand((1, 3))\n\n# Summation\ntorch.einsum(\"ij->\", x) \n\n# Matrix-Vector Multiplication, just specify the demensions as we would like\ntorch.einsum(\"ij,kj->ik\", x, v)\n\n# Matrix-Matrix Multiplication\ntorch.einsum(\"ij,kj->ik\", x, x)\n\n# Batch Matrix Multiplication\nZ1 = torch.rand((2, 5, 3))\nZ2 = torch.rand((2, 3, 4))\ntorch.einsum(\"ijk,ikl->ijl\", z1, z2)\n```\n\n","tags":["torch"],"categories":["source code"]},{"title":"Convnext","url":"/2022/03/30/convnext/","content":"\n# 网络结构\n\n## Block\n\n<img src=\"convnext/block.png\" alt=\"convnext\" style=\"zoom:33%;\" />\n\n``` python\nclass Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n    Args:\n        dim (int): Number of input channels.\n        drop_rate (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6, data_format=\"channels_last\")\n        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        # 对应Layer Scale\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_rate) if drop_rate > 0. else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            # 对每个通道数据缩放\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]\n\n        x = shortcut + self.drop_path(x)\n        return x\n```\n\n# 整体结构\n\n<div style=\"display:table; width:100%;\">\n    <div style=\"display:table-row\">\n        <div style=\"display:table-cell; width:6%\"><img src=\"convnext/network.png\"></div>\n        <div style=\"display:table-cell; width:20%;\"><img src=\"convnext/config.png\"></div>\n    </div>\n</div>\n\n\n```python\nclass ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n    def __init__(self, in_chans: int = 3, num_classes: int = 1000, depths: list = None,\n                 dims: list = None, drop_path_rate: float = 0., layer_scale_init_value: float = 1e-6,\n                 head_init_scale: float = 1.):\n        super().__init__()\n        # stem and 3 intermediate downsampling conv layers\n        self.downsample_layers = nn.ModuleList()  \n        # dims[0]对应每个stage输入特征通道数\n        stem = nn.Sequential(nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n                             LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"))\n        self.downsample_layers.append(stem)\n        \n        # 对应stage2-stage4前的3个downsample，downsample：LN+Conv2d k2,s2\n        for i in range(3):\n            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                                             nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2))\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple blocks\n        # depth对应上图B中参数\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0 #计数，当前blcok之前已经构造好的block个数\n        # 构建每个stage中堆叠的block，j在当前block中构建第几个blcok\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)\n                  for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n            \n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.trunc_normal_(m.weight, std=0.2)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n\n        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n    \ndef convnext_tiny(num_classes: int):\n# https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\nmodel = ConvNeXt(depths=[3, 3, 9, 3],\n                 dims=[96, 192, 384, 768],\n                 num_classes=num_classes)\n    return model\n```\n\n## v5对应的配置参数\n\n```markd\n# YOLOv5 backbone\n# [from, number, module, args]\n# 3：初始通道数，后续并没有用到\nbackbone:\n  [[-1, 1, ConvNeXt_Block, [96, 0, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [192, 1, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [384, 2, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n   [-1, 1, ConvNeXt_Block, [768, 3, 3, [3, 3, 9, 3], [96, 192, 384, 768]]],\n  ]\n```\n\n\n\n\n\n\n\n\n\n\n\n​    \n\n\n\n\n\n\n\n","tags":["code"],"categories":["paper"]}]